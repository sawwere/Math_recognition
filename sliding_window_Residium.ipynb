{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c62fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from PIL import Image, ImageOps, ImageFont, ImageDraw\n",
    "from torchvision import transforms\n",
    "\n",
    "from imutils.object_detection import non_max_suppression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c3631a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classes = ['(', ')', ',', '-', \n",
    "#                    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "#                    'A', '[', 'α', 'and', 'β', '∃', 'F', '∀', \n",
    "#                    'γ', 'λ', 'μ', 'ω', 'or', 'φ', '→', \n",
    "#                    'σ', 'sqrt', 'θ', 'v', 'x', 'y', 'z']\n",
    "\n",
    "classes = ['(', ')', '+', ',', '-', \n",
    "                     '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                     'A', 'F','α','and','β', 'delta', '∃', '∀',\n",
    "                     'γ', 'λ', 'μ', 'not','ω','or','φ','pi','psi','→',\n",
    "                     'σ','tau','θ','v','x', 'y', 'z']\n",
    "\n",
    "NUM_CLASSES = len(classes)\n",
    "\n",
    "\n",
    "def map_pred(ind):\n",
    "    if ind < NUM_CLASSES:\n",
    "        return classes[ind]\n",
    "    return 'ERROR MAPPIMG'\n",
    "\n",
    "MODEL_PATH = 'models/mathnet/mathnet20.ml'\n",
    "NUM_CLASSES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f64ed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidiumBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_percentage, is_reducer=True):\n",
    "        super(ResidiumBlock, self).__init__()\n",
    "        self.dropout_percentage = dropout_percentage\n",
    "        self.is_reducer = is_reducer\n",
    "        if self.is_reducer:\n",
    "            self.conv1 = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                                         kernel_size=3, padding=1, stride=2)\n",
    "        else:\n",
    "            self.conv1 = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                                         kernel_size=3, padding=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=out_channels, out_channels=out_channels, \n",
    "                                     kernel_size=3, padding=1)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.act1  = torch.nn.ReLU()\n",
    "        \n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=out_channels, out_channels=out_channels, \n",
    "                                     kernel_size=3, padding=1)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.conv4 = torch.nn.Conv2d(in_channels=out_channels, out_channels=out_channels, \n",
    "                                     kernel_size=3, padding=1)\n",
    "        self.bn4 = torch.nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.act2  = torch.nn.ReLU()\n",
    "        #self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        self.dropout = nn.Dropout(p=self.dropout_percentage)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        if not self.is_reducer:\n",
    "            x += identity\n",
    "        x = self.act1(x)\n",
    "        \n",
    "        identity = x\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x += identity\n",
    "        x = self.act2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31402c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathNet(torch.nn.Module):\n",
    "    def __init__(self, out_size=NUM_CLASSES):\n",
    "        super(MathNet, self).__init__()\n",
    "        self.dropout_percentage = 0.25\n",
    "        \n",
    "        # 28x28x1 -> 28x28x64\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.block1 = ResidiumBlock(64, 64, self.dropout_percentage, False)\n",
    "        self.block2 = ResidiumBlock(64, 128, self.dropout_percentage)\n",
    "        self.block3 = ResidiumBlock(128, 256, self.dropout_percentage)\n",
    "        #self.block4 = ResidiumBlock(256, 512, self.dropout_percentage)\n",
    "\n",
    "        self.pool3 = torch.nn.AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
    "        self.dropout3 = nn.Dropout(p=self.dropout_percentage)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(256, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        #x = self.block4(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = x.view(x.size(0), x.size(1) * x.size(2) * x.size(3))\n",
    "        x = self.fc1(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3377f4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels=1, out_channels=6, kernel_size=5, padding=2)\n",
    "        self.act1  = torch.nn.ReLU()\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "       \n",
    "        self.conv2_1 = torch.nn.Conv2d(\n",
    "            in_channels=6, out_channels=12, kernel_size=3, padding=0)\n",
    "        self.conv2_2 = torch.nn.Conv2d(\n",
    "            in_channels=12, out_channels=16, kernel_size=3, padding=0)\n",
    "        self.act2  = torch.nn.ReLU()\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc1   = torch.nn.Linear(5 * 5 * 16, 120)\n",
    "        self.act3  = torch.nn.ReLU()\n",
    "        \n",
    "        self.fc2   = torch.nn.Linear(120, 84)\n",
    "        self.act4  = torch.nn.Tanh()\n",
    "        \n",
    "        self.fc3   = torch.nn.Linear(84, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2_1(x)\n",
    "        x = self.conv2_2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = x.view(x.size(0), x.size(1) * x.size(2) * x.size(3))\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2c5e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed8b8dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(torch.nn.Module):\n",
    "    def __init__(self, block, image_channels, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        layers = [2, 2, 2, 2]\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # ResNetLayers\n",
    "        self.layer1 = self.make_layers(block, layers[0], intermediate_channels=64, stride=1)\n",
    "        self.layer2 = self.make_layers(block, layers[1], intermediate_channels=128, stride=2)\n",
    "        self.layer3 = self.make_layers(block, layers[2], intermediate_channels=256, stride=2)\n",
    "        self.layer4 = self.make_layers(block, layers[3], intermediate_channels=512, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def make_layers(self, block, num_residual_blocks, intermediate_channels, stride):\n",
    "        layers = []\n",
    "\n",
    "        identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels, \n",
    "                                                      intermediate_channels, \n",
    "                                                      kernel_size=1, \n",
    "                                                      stride=stride),\n",
    "                                            nn.BatchNorm2d(intermediate_channels))\n",
    "        layers.append(block(self.in_channels, intermediate_channels, identity_downsample, stride))\n",
    "        self.in_channels = intermediate_channels # 256\n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(block(self.in_channels, intermediate_channels)) # 256 -> 64, 64*4 (256) again\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cfa67be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18(img_channels=3, num_classes=NUM_CLASSES):\n",
    "    return ResNet(Block, img_channels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8af6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "434ef9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUDA_LAUNCH_BLOCKING=1\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e90c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "(H, W) = (400, 400)\n",
    "class SlidingWindowObjectDetection():\n",
    "    def __init__(self, pretrained_classifier_path, kwargs):\n",
    "        self.model = MathNet()\n",
    "        self.model.load_state_dict(torch.load(pretrained_classifier_path))\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = self.model.to(device)\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def tttt(self, image, step, ws):\n",
    "        potential = []\n",
    "        for y in range(0, image.shape[0] - ws[1], step):\n",
    "            for x in range(0, image.shape[1] - ws[0], step):\n",
    "                crop_img = image[y:y+28, x:x+28]\n",
    "                #print(type(image), type(crop_img))\n",
    "                crop_tensor = transforms.ToTensor()\n",
    "                \n",
    "               \n",
    "                \n",
    "                thresh = 120\n",
    "                ret,thresh_img = cv2.threshold(crop_img, thresh, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "                #find contours\n",
    "                contours, hierarchy = cv2.findContours(thresh_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                img_contours = np.uint8(np.zeros((crop_img.shape[0],crop_img.shape[1])))\n",
    "                cv2.drawContours(img_contours, contours, -1, (255,255,255), 1)\n",
    "                \n",
    "                if (len(contours) > 1):\n",
    "                    #print(x,y, len(contours))\n",
    "                    #img = Image.fromarray(crop_img.astype('uint8'))\n",
    "                    #img = Image.fromarray(img_contours.astype('uint8'))\n",
    "                    #display(img)\n",
    "                    potential.append(crop_img)\n",
    "        return potential\n",
    "    \n",
    "    def predict(self, lst):\n",
    "        res = []\n",
    "        for image in lst:\n",
    "            img = Image.fromarray(image.astype('uint8'))\n",
    "            convert_tensor = transforms.Compose([\n",
    "                transforms.Resize((28,28)),\n",
    "                transforms.Grayscale(1),\n",
    "                transforms.ToTensor()\n",
    "\n",
    "            ])\n",
    "\n",
    "            \n",
    "            x_image = convert_tensor(img)\n",
    "            x_image = x_image.unsqueeze(0).float()\n",
    "            x_image = x_image.to(device)\n",
    "\n",
    "            preds = self.model(x_image) \n",
    "            prob = preds.max()\n",
    "            if prob >= self.kwargs['MIN_CONF']:\n",
    "                print(prob.item(), (map_pred(preds.argmax()), preds))\n",
    "                img = Image.fromarray(image.astype('uint8'))\n",
    "                display(img)\n",
    "            \n",
    "                res.append((map_pred(preds.argmax()), preds))\n",
    "\n",
    "        \n",
    "    def visualize_rois(self, rois):\n",
    "        fig, axes = plt.subplots(1, len(rois), figsize=(20, 6))\n",
    "        for ax, roi in zip(axes, rois):\n",
    "            ax.imshow(roi, cmap='gray')\n",
    "\n",
    "    \n",
    "    def get_preds(self, rois, locs):\n",
    "        rois = np.array(rois, dtype=\"float32\")\n",
    "        preds = self.predict(rois)\n",
    "        #preds = list(zip(preds.argmax(axis=1).tolist(), preds.max(axis=1).tolist()))\n",
    "        res = []\n",
    "        for i in range(0, len(preds)):\n",
    "            res.append((map_pred(preds[i].argmax()), preds[i].max()))\n",
    "        #print(res)\n",
    "        labels = {}\n",
    "\n",
    "        for (i, p) in enumerate(res):\n",
    "            (label, prob) = p\n",
    "            if prob >= self.kwargs['MIN_CONF']:\n",
    "                box = locs[i]\n",
    "                L = labels.get(label, [])\n",
    "                L.append((box, prob))\n",
    "                labels[label] = L\n",
    "        return preds, labels\n",
    "    \n",
    "    def apply_nms(self, labels):\n",
    "        nms_labels = {}\n",
    "        for label in sorted(labels.keys()):\n",
    "            boxes = np.array([p[0] for p in labels[label]])\n",
    "            proba = np.array([p[1] for p in labels[label]])\n",
    "            boxes = non_max_suppression(boxes, proba)\n",
    "            nms_labels[label] = boxes.tolist()\n",
    "        return nms_labels\n",
    "            \n",
    "    def visualize_preds(self, img, nms_labels):\n",
    "        for label in sorted(nms_labels.keys()):\n",
    "            clone = img.copy()\n",
    "            fig, ax = plt.subplots(figsize=(20, 6))\n",
    "            boxes = nms_labels[label]\n",
    "            for (startX, startY, endX, endY) in boxes:\n",
    "                cv2.rectangle(clone, (startX, startY), (endX, endY), (255, 255, 255), 1)\n",
    "                y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "                cv2.putText(clone, str(label), (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 255, 255), 1)\n",
    "            ax.imshow(clone, cmap='gray')\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        potential = self.tttt(img, self.kwargs['WIN_STEP'], self.kwargs['ROI_SIZE'])\n",
    "        self.predict(potential)\n",
    "        \n",
    "        rois, locs = self.get_rois_and_locs()\n",
    "        print(6666666666666)\n",
    "#         if self.kwargs['VIZ_ROIS']:\n",
    "#             self.visualize_rois(rois)\n",
    "        \n",
    "        preds, labels = self.get_preds(rois, locs)\n",
    "        print(777777777777)\n",
    "        nms_labels = self.apply_nms(labels)\n",
    "        print(888888888888)\n",
    "        if self.kwargs['VISUALIZE']:\n",
    "            self.visualize_preds(img, nms_labels)\n",
    "        print(999999999999)\n",
    "        return nms_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70c2aa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(\n",
    "    PYR_SCALE=1.25,\n",
    "    WIN_STEP=3,\n",
    "    ROI_SIZE=(21, 21),\n",
    "    INPUT_SIZE=(28, 28),\n",
    "    VISUALIZE=True,\n",
    "    MIN_CONF=0.2,\n",
    "    VIZ_ROIS=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f0e2b55",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 8\u001b[0m\n\u001b[0;32m      2\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(IMAGE_NAME)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# cv2.imshow('contours', image)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# cv2.waitKey()\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# cv2.destroyAllWindows()\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#image = resize_image(image, 256)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m img_grey \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#set a thresh\u001b[39;00m\n\u001b[0;32m     11\u001b[0m thresh \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m120\u001b[39m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "IMAGE_NAME = '5.jpg'\n",
    "image = cv2.imread(IMAGE_NAME)\n",
    "# cv2.imshow('contours', image)\n",
    "# cv2.waitKey()\n",
    "# cv2.destroyAllWindows()\n",
    "#image = resize_image(image, 256)\n",
    "\n",
    "img_grey = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#set a thresh\n",
    "thresh = 120\n",
    "\n",
    "#get threshold image\n",
    "ret,thresh_img = cv2.threshold(img_grey, thresh, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "#find contours\n",
    "contours, hierarchy = cv2.findContours(thresh_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "img_contours = np.uint8(np.zeros((image.shape[0],image.shape[1])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f6a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.drawContours(img_contours, contours, -1, (255,255,255), 1)\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f8d7d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sw = SlidingWindowObjectDetection(MODEL_PATH, kwargs)\n",
    "IMAGE_NAME = '5.jpg'\n",
    "image = cv2.imread(IMAGE_NAME)\n",
    "img_grey = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "#sw(img_grey)\n",
    "print('END')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344ea2c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f789fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, dst):\n",
    "#     # соотношение сторон: ширина, делённая на ширину оригинала\n",
    "#     ratio = float(dwt / image.shape[1])\n",
    "#     print(ratio)\n",
    "#     # желаемая высота: высота, умноженная на соотношение сторон\n",
    "#     dht = int(image.shape[0] * ratio)\n",
    "#     print(image.shape[0] * ratio)\n",
    "#     dim = (dwt, dht)  # итоговые размеры\n",
    "#     print(dim)\n",
    "    # Масштабируем картинку\n",
    "    # Подготовим новые размеры\n",
    "    x = float(dst) / image.shape[1]\n",
    "    y = float(dst) / image.shape[0]\n",
    "    # уменьшаем изображение до подготовленных размеров\n",
    "    return cv2.resize(image, (0,0), fx=x,fy=y, interpolation = cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af995677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_letter(image, dst):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3e62ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Letter:\n",
    "    def __init__(self, x, y, w, h, img):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.width = w\n",
    "        self.height = h\n",
    "        self.image = img\n",
    "        \n",
    "        self.line = 0\n",
    "        \n",
    "        self.bottom = self.y + self.height\n",
    "        self.top = self.y\n",
    "        self.left = self.x\n",
    "        self.right = self.x + self.width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0d158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letters_extract(image_file, out_size=224):\n",
    "    img = cv2.imread(image_file)\n",
    "    output = img.copy()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #set a thresh\n",
    "    thresh = 100\n",
    "    ret, thresh_img = cv2.threshold(gray, thresh, 255, cv2.THRESH_BINARY)\n",
    "    img_erode = cv2.erode(thresh_img, np.ones((3, 3), np.uint8), iterations=3)\n",
    "#     cv2.imshow('MyPhoto', img_erode)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "    \n",
    "\n",
    "    # Get contours\n",
    "    contours, hierarchy = cv2.findContours(img_erode, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)       \n",
    "    img_contours = np.uint8(np.zeros((img.shape[0],img.shape[1])))\n",
    "    cv2.drawContours(img_contours, contours, -1, (255,255,255), 1)\n",
    "#     cv2.imshow('MyPhoto', img_contours )\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "       \n",
    "    # Filter contours\n",
    "    mask = np.uint8(np.zeros((img.shape[0],img.shape[1])))\n",
    "    for idx, contour in enumerate(contours):\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        if cv2.contourArea(contour) > 100 and cv2.contourArea(contour) < 10000:\n",
    "            cv2.drawContours(mask, [contour], 0, (255), -1)\n",
    "        else:\n",
    "            pass\n",
    "#             dummy = np.uint8(np.zeros((img.shape[0],img.shape[1])))\n",
    "#             cv2.drawContours(dummy, contour, -1, (255,255,255), 1)\n",
    "#             print(w*h, cv2.contourArea(contour))\n",
    "#             cv2.imshow(str(idx), dummy )\n",
    "#             cv2.waitKey(0)\n",
    "#             cv2.destroyAllWindows()\n",
    "            \n",
    "#     # apply the mask to the original image\n",
    "    result = cv2.bitwise_and(img,img, mask= mask)   \n",
    "#     cv2.imshow('result', result )\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "    \n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    img_contours = np.uint8(np.zeros((img.shape[0],img.shape[1])))\n",
    "    cv2.drawContours(img_contours, contours, -1, (255,255,255), 1)\n",
    "#     cv2.imshow('MyPhoto', img_contours )\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "    \n",
    "    letters = []\n",
    "    for idx, contour in enumerate(contours):\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        #print(\"R\", idx, x, y, w, h, cv2.contourArea(contour), hierarchy[0][idx])\n",
    "        if hierarchy[0][idx][3] != -1:\n",
    "            continue\n",
    "#         mask_contour = img_contours[y:y+h, x:x+w]\n",
    "#         mask_contour[y_pos:y_pos + h, 0:w] = crop_img\n",
    "#         cv2.drawContours(mask_contour, contour, -1, (255,255,255), 1)\n",
    "#         cv2.imshow('MyPhoto', mask_contour)\n",
    "#         cv2.waitKey(0)\n",
    "#         cv2.destroyAllWindows()\n",
    "\n",
    "#         cv2.imwrite('mask.png', img * max(img - 100, 0) * 255)\n",
    "#         ii = cv2.imread('mask.png')\n",
    "#         cv2.imshow('MyPhoto', ii )\n",
    "#         cv2.waitKey(0)\n",
    "#         cv2.destroyAllWindows()\n",
    "        crop_img = img_erode[y:y+h, x:x+w]\n",
    "        #crop_img = thresh_img[y:y+h, x:x+w]\n",
    "        size_max = max(w, h)\n",
    "        letter_square = 255 * np.ones(shape=[size_max, size_max], dtype=np.uint8)\n",
    "        if w > h:\n",
    "            y_pos = size_max//2 - h//2\n",
    "            letter_square[y_pos:y_pos + h, 0:w] = crop_img\n",
    "        elif w < h:\n",
    "            x_pos = size_max//2 - w//2\n",
    "            letter_square[0:h, x_pos:x_pos + w] = crop_img\n",
    "        else:\n",
    "            letter_square = crop_img\n",
    "        \n",
    "        x,y,w,h = cv2.boundingRect(contour)\n",
    "        rect = cv2.rectangle(output, (x, y), (x + w, y + h), (0, 255,0), 2)      \n",
    "        inverted = cv2.bitwise_not(letter_square)\n",
    "        \n",
    "        letter = Letter(x,y,w,h,letter_square)\n",
    "        letters.append(letter)\n",
    "\n",
    "    letters.sort(key=lambda ll: (ll.y, ll.x), reverse=False)\n",
    "    return letters, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2025e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_size(lst):\n",
    "    if len(lst) <= 0:\n",
    "        return (0, 0)\n",
    "    avg_w = 0\n",
    "    avg_h = 0\n",
    "    for letter in lst:\n",
    "        avg_w += letter.width\n",
    "        avg_h += letter.height\n",
    "    avg_w /= len(lst)\n",
    "    avg_h /= len(lst)\n",
    "    \n",
    "    return (avg_w, avg_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf06bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_str(model, image_file):\n",
    "    letters, output = letters_extract(image_file)   \n",
    "    output = Image.fromarray(output.astype('uint8'))\n",
    "    print('SHAPE: ', np.array(letters,dtype=object).shape)\n",
    "    s_out = \"\"\n",
    "    if len(letters) == 0:\n",
    "        return \"Found nothing\"\n",
    "    (avg_w, avg_h) = average_size(letters)\n",
    "    print((avg_w, avg_h))\n",
    "    \n",
    "    # True sorting by Y axis\n",
    "    line = 0\n",
    "    for i in range (1, len(letters)):\n",
    "        if letters[i].top > letters[i-1].bottom:\n",
    "            line += 1\n",
    "        letters[i].line = line\n",
    "    letters.sort(key=lambda ll: (ll.line, ll.x), reverse=False)  \n",
    "    \n",
    "    prev_loc = (letters[0].x, letters[0].y)\n",
    "    prev_size = (letters[0].width, letters[0].height)\n",
    "    prev_line = letters[0].line\n",
    "    print()\n",
    "    for i in range(len(letters)):\n",
    "        img = Image.fromarray(letters[i].image.astype('uint8'))\n",
    "        convert_tensor = transforms.Compose([\n",
    "            transforms.Resize((28,28)),\n",
    "            transforms.Grayscale(1),\n",
    "            transforms.ToTensor()\n",
    "\n",
    "        ])        \n",
    "        x_image = convert_tensor(img)\n",
    "        aaa = transforms.ToPILImage()\n",
    "        display(aaa(x_image))\n",
    "        x_image = x_image.unsqueeze(0).float()\n",
    "        x_image = x_image.to(device)\n",
    "        pred = model(x_image) \n",
    "        pred_arg_max = pred.argmax().item()\n",
    "        mapped = map_pred(pred_arg_max)\n",
    "        \n",
    "        #cv2.putText(output, mapped+' '+\"{:.2f}\".format(pred.max().item()), (letters[i].x, letters[i].y), cv2.FONT_HERSHEY_COMPLEX, 0.45, (0, 0, 0), 1)\n",
    "\n",
    "        # Draw non-ascii text onto image\n",
    "        font = ImageFont.truetype(\"ARIALUNI.TTF\", 24, encoding=\"unic\")\n",
    "        draw = ImageDraw.Draw(output)\n",
    "        draw.text((letters[i].x, letters[i].y), mapped+' '+\"{:.2f}\".format(pred.max().item()), font=font)\n",
    "        \n",
    "        x = letters[i].x\n",
    "        y = letters[i].y\n",
    "        size = (letters[i].width, letters[i].height)\n",
    "        if (letters[i].line >  prev_line):\n",
    "            s_out += \"\\n\"\n",
    "            prev_line = letters[i].line\n",
    "        prev_loc, prev_size = (x,y), size\n",
    "        s_out += mapped + ' '\n",
    "        print(letters[i].image.shape, \"{:.2f}\".format(pred.max().item()), mapped)\n",
    "    #output = Image.fromarray(output.astype('uint8'))\n",
    "    display(output)\n",
    "    return s_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38953a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MathNet()\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print('EVALUATION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf68e22",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = img_to_str(model, '5.jpg')\n",
    "print('RESULT:')\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef2b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c83a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abracadabra():\n",
    "    img = cv2.imread('real.jpg')\n",
    "    cv2.imshow('MyPhoto', img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #set a thresh\n",
    "    thresh = 100\n",
    "    ret, thresh_img = cv2.threshold(gray, thresh, 255, cv2.THRESH_BINARY)\n",
    "    img_erode = cv2.erode(thresh_img, np.ones((3, 3), np.uint8), iterations=1)\n",
    "\n",
    "    # Get contours\n",
    "    contours, hierarchy = cv2.findContours(img_erode, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    cv2.imshow('contours', resize_image(thresh_img, 224))\n",
    "    #     cv2.waitKey()\n",
    "    #     cv2.destroyAllWindows()\n",
    "\n",
    "    output = img.copy()\n",
    "\n",
    "    img_contours = np.uint8(np.zeros((img.shape[0],img.shape[1])))\n",
    "    cv2.drawContours(img_contours, contours, -1, (255,255,255), 1)\n",
    "    cv2.imshow('origin', resize_image(img, 224)) # выводим итоговое изображение в окно\n",
    "    #cv2.imshow('gray', resize_image(gray, 224)) # выводим итоговое изображение в окно\n",
    "    cv2.imshow('res', resize_image(img_contours, 224)) # выводим итоговое изображение в окно\n",
    "\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80cc3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3262b13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MathNet()\n",
    "model.load_state_dict(torch.load('models//mathnet/mathnet40.ml'))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "img = Image.open(\"0.jpg\")\n",
    "#print(img.shape)\n",
    "convert_tensor = transforms.Compose([\n",
    "    transforms.Resize((28,28)),\n",
    "    transforms.Grayscale(1),\n",
    "    transforms.ToTensor()\n",
    "\n",
    "])        \n",
    "x_image = convert_tensor(img)\n",
    "\n",
    "aaa = transforms.ToPILImage()\n",
    "display(aaa(x_image))\n",
    "\n",
    "x_image = x_image.unsqueeze(0).float()\n",
    "x_image = x_image.to(device)\n",
    "\n",
    "pred = model(x_image) \n",
    "print(pred*10, map_pred(pred.argmax().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4af59fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a15b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28961b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a9881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7603e35a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
