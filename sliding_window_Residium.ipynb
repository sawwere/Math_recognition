{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c62fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from PIL import Image, ImageOps, ImageFont, ImageDraw\n",
    "from torchvision import transforms\n",
    "\n",
    "from imutils.object_detection import non_max_suppression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "import models.MathNet as mnt\n",
    "import models.MathNet56 as mnt56\n",
    "from models.MathNetFactory import MathNetFactory\n",
    "from SlidingWindow import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c3631a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 56\n",
    "\n",
    "NUM_CLASSES = mnt.NUM_CLASSES\n",
    "factory = MathNetFactory()\n",
    "factory.SetModel(IMAGE_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98d0a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUDA_LAUNCH_BLOCKING=1\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3377f4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels=1, out_channels=6, kernel_size=5, padding=2)\n",
    "        self.act1  = torch.nn.ReLU()\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "       \n",
    "        self.conv2_1 = torch.nn.Conv2d(\n",
    "            in_channels=6, out_channels=12, kernel_size=3, padding=0)\n",
    "        self.conv2_2 = torch.nn.Conv2d(\n",
    "            in_channels=12, out_channels=16, kernel_size=3, padding=0)\n",
    "        self.act2  = torch.nn.ReLU()\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc1   = torch.nn.Linear(5 * 5 * 16, 120)\n",
    "        self.act3  = torch.nn.ReLU()\n",
    "        \n",
    "        self.fc2   = torch.nn.Linear(120, 84)\n",
    "        self.act4  = torch.nn.Tanh()\n",
    "        \n",
    "        self.fc3   = torch.nn.Linear(84, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2_1(x)\n",
    "        x = self.conv2_2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = x.view(x.size(0), x.size(1) * x.size(2) * x.size(3))\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "434ef9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Letter:\n",
    "    def __init__(self, x, y, w, h, img):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.width = w\n",
    "        self.height = h\n",
    "        self.image = img\n",
    "        \n",
    "        self.line = 0\n",
    "        \n",
    "        self.bottom = self.y + self.height\n",
    "        self.top = self.y\n",
    "        self.left = self.x\n",
    "        self.right = self.x + self.width\n",
    "\n",
    "        self.value = 'None'\n",
    "        \n",
    "    def resize():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70c2aa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(\n",
    "        PYR_SCALE=2.25,\n",
    "        WIN_STEP=16,\n",
    "        ROI_SIZE=(48, 48),\n",
    "        INPUT_SIZE=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        VISUALIZE=True,\n",
    "        MIN_CONF=3.05,\n",
    "        DEBUG=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74f8d7d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regions_of_interest =  353\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (512x4x4). Calculated output size: (512x0x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m IMAGE_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEST//real5.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(IMAGE_NAME)\n\u001b[1;32m----> 4\u001b[0m output \u001b[38;5;241m=\u001b[39m sw(image)\n\u001b[0;32m      6\u001b[0m letters \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m letter \u001b[38;5;129;01min\u001b[39;00m letters:\n",
      "File \u001b[1;32mt:\\my_programs\\Math_recognition\\SlidingWindow.py:254\u001b[0m, in \u001b[0;36mSlidingWindow.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregions_of_interest = \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(regions_of_interest))\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m#regions_of_interest = self.get_exact_locations(regions_of_interest)\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m#self.add_spaces_to_letter(regions_of_interest)\u001b[39;00m\n\u001b[1;32m--> 254\u001b[0m (regions_of_interest, preds) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(regions_of_interest)\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDEBUG\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mletters predicted = \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(regions_of_interest))\n",
      "File \u001b[1;32mt:\\my_programs\\Math_recognition\\SlidingWindow.py:189\u001b[0m, in \u001b[0;36mSlidingWindow.predict\u001b[1;34m(self, letters)\u001b[0m\n\u001b[0;32m    186\u001b[0m x_image \u001b[38;5;241m=\u001b[39m x_image\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    187\u001b[0m x_image \u001b[38;5;241m=\u001b[39m x_image\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 189\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x_image) \n\u001b[0;32m    190\u001b[0m prob \u001b[38;5;241m=\u001b[39m predicted\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m#print(prob)\u001b[39;00m\n",
      "File \u001b[1;32me:\\Programs\\Anaconda3\\envs\\MyPyTorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Programs\\Anaconda3\\envs\\MyPyTorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mt:\\my_programs\\Math_recognition\\models\\MathNet56.py:94\u001b[0m, in \u001b[0;36mMathNet56.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     92\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock3(x)\n\u001b[0;32m     93\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock4(x)\n\u001b[1;32m---> 94\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool3(x)\n\u001b[0;32m     96\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m     97\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)  \n",
      "File \u001b[1;32me:\\Programs\\Anaconda3\\envs\\MyPyTorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Programs\\Anaconda3\\envs\\MyPyTorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Programs\\Anaconda3\\envs\\MyPyTorch\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:635\u001b[0m, in \u001b[0;36mAvgPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 635\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mavg_pool2d(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    636\u001b[0m                         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mceil_mode, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount_include_pad, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdivisor_override)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given input size: (512x4x4). Calculated output size: (512x0x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "sw = SlidingWindow(factory.LoadModel(), kwargs)\n",
    "IMAGE_NAME = 'TEST//real5.jpg'\n",
    "image = cv2.imread(IMAGE_NAME)\n",
    "output = sw(image)\n",
    "\n",
    "letters = output[1]\n",
    "for letter in letters:\n",
    "    letter_image = Image.fromarray(letter.image.astype('uint8'))\n",
    "    print(letter.value)\n",
    "    display(letter_image)\n",
    "print('END')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b78ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfcf41c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f789fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, dst):\n",
    "#     # соотношение сторон: ширина, делённая на ширину оригинала\n",
    "#     ratio = float(dwt / image.shape[1])\n",
    "#     print(ratio)\n",
    "#     # желаемая высота: высота, умноженная на соотношение сторон\n",
    "#     dht = int(image.shape[0] * ratio)\n",
    "#     print(image.shape[0] * ratio)\n",
    "#     dim = (dwt, dht)  # итоговые размеры\n",
    "#     print(dim)\n",
    "    # Масштабируем картинку\n",
    "    # Подготовим новые размеры\n",
    "    x = float(dst) / image.shape[1]\n",
    "    y = float(dst) / image.shape[0]\n",
    "    # уменьшаем изображение до подготовленных размеров\n",
    "    return cv2.resize(image, (0,0), fx=x,fy=y, interpolation = cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3e62ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Letter:\n",
    "    def __init__(self, x, y, w, h, img):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.width = w\n",
    "        self.height = h\n",
    "        self.image = img\n",
    "        \n",
    "        self.line = 0\n",
    "        \n",
    "        self.bottom = self.y + self.height\n",
    "        self.top = self.y\n",
    "        self.left = self.x\n",
    "        self.right = self.x + self.width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0d158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letters_extract(image_file):\n",
    "    img = cv2.imread(image_file)\n",
    "    output = img.copy()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #set a thresh\n",
    "    thresh = 100\n",
    "    ret, thresh_img = cv2.threshold(gray, thresh, 255, cv2.THRESH_BINARY)\n",
    "    img_erode = thresh_img#cv2.erode(thresh_img, np.ones((3, 3), np.uint8), iterations=3)\n",
    "#     cv2.imshow('MyPhoto', img_erode)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "    \n",
    "\n",
    "    # Get contours\n",
    "    contours, hierarchy = cv2.findContours(img_erode, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)       \n",
    "    img_contours = np.uint8(np.zeros((img.shape[0],img.shape[1])))\n",
    "    cv2.drawContours(img_contours, contours, -1, (255,255,255), 1)\n",
    "    cv2.imshow('MyPhoto', img_contours )\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "       \n",
    "    # Filter contours\n",
    "    mask = np.uint8(np.zeros((img.shape[0],img.shape[1])))\n",
    "    for idx, contour in enumerate(contours):\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        if cv2.contourArea(contour) > 1 and cv2.contourArea(contour) < 10000:\n",
    "            cv2.drawContours(mask, [contour], 0, (255), -1)\n",
    "        else:\n",
    "            pass\n",
    "#             dummy = np.uint8(np.zeros((img.shape[0],img.shape[1])))\n",
    "#             cv2.drawContours(dummy, contour, -1, (255,255,255), 1)\n",
    "#             print(w*h, cv2.contourArea(contour))\n",
    "#             cv2.imshow(str(idx), dummy )\n",
    "#             cv2.waitKey(0)\n",
    "#             cv2.destroyAllWindows()\n",
    "            \n",
    "#     # apply the mask to the original image\n",
    "    result = cv2.bitwise_and(img,img, mask= mask)   \n",
    "    cv2.imshow('result', result )\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    img_contours = np.uint8(np.zeros((img.shape[0],img.shape[1])))\n",
    "    cv2.drawContours(img_contours, contours, -1, (255,255,255), 1)\n",
    "    cv2.imshow('img_contours', img_contours )\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    letters = []\n",
    "    for idx, contour in enumerate(contours):\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        #print(\"R\", idx, x, y, w, h, cv2.contourArea(contour), hierarchy[0][idx])\n",
    "        if hierarchy[0][idx][3] != -1:\n",
    "            continue\n",
    "#         mask_contour = img_contours[y:y+h, x:x+w]\n",
    "#         mask_contour[y_pos:y_pos + h, 0:w] = crop_img\n",
    "#         cv2.drawContours(mask_contour, contour, -1, (255,255,255), 1)\n",
    "#         cv2.imshow('MyPhoto', mask_contour)\n",
    "#         cv2.waitKey(0)\n",
    "#         cv2.destroyAllWindows()\n",
    "\n",
    "#         cv2.imwrite('mask.png', img * max(img - 100, 0) * 255)\n",
    "#         ii = cv2.imread('mask.png')\n",
    "#         cv2.imshow('MyPhoto', ii )\n",
    "#         cv2.waitKey(0)\n",
    "#         cv2.destroyAllWindows()\n",
    "        crop_img = img_erode[y:y+h, x:x+w]\n",
    "        #crop_img = thresh_img[y:y+h, x:x+w]\n",
    "        size_max = max(w, h)\n",
    "        letter_square = 255 * np.ones(shape=[size_max, size_max], dtype=np.uint8)\n",
    "        if w > h:\n",
    "            y_pos = size_max//2 - h//2\n",
    "            letter_square[y_pos:y_pos + h, 0:w] = crop_img\n",
    "        elif w < h:\n",
    "            x_pos = size_max//2 - w//2\n",
    "            letter_square[0:h, x_pos:x_pos + w] = crop_img\n",
    "        else:\n",
    "            letter_square = crop_img\n",
    "        \n",
    "        x,y,w,h = cv2.boundingRect(contour)\n",
    "        rect = cv2.rectangle(output, (x, y), (x + w, y + h), (0, 255,0), 2)      \n",
    "        inverted = cv2.bitwise_not(letter_square)\n",
    "        cv2.imshow('inverted', inverted )\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        letter = Letter(x,y,w,h,letter_square)\n",
    "        letters.append(letter)\n",
    "\n",
    "    letters.sort(key=lambda ll: (ll.y, ll.x), reverse=False)\n",
    "    return letters, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2025e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_size(lst):\n",
    "    if len(lst) <= 0:\n",
    "        return (0, 0)\n",
    "    avg_w = 0\n",
    "    avg_h = 0\n",
    "    for letter in lst:\n",
    "        avg_w += letter.width\n",
    "        avg_h += letter.height\n",
    "    avg_w /= len(lst)\n",
    "    avg_h /= len(lst)\n",
    "    \n",
    "    return (avg_w, avg_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf06bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_str(model, image_file):\n",
    "    letters, output = letters_extract(image_file)   \n",
    "    output = Image.fromarray(output.astype('uint8'))\n",
    "    print('SHAPE: ', np.array(letters,dtype=object).shape)\n",
    "    s_out = \"\"\n",
    "    if len(letters) == 0:\n",
    "        return \"Found nothing\"\n",
    "    (avg_w, avg_h) = average_size(letters)\n",
    "    print((avg_w, avg_h))\n",
    "    \n",
    "    # True sorting by Y axis\n",
    "    line = 0\n",
    "    for i in range (1, len(letters)):\n",
    "        if letters[i].top > letters[i-1].bottom:\n",
    "            line += 1\n",
    "        letters[i].line = line\n",
    "    letters.sort(key=lambda ll: (ll.line, ll.x), reverse=False)  \n",
    "    \n",
    "    prev_loc = (letters[0].x, letters[0].y)\n",
    "    prev_size = (letters[0].width, letters[0].height)\n",
    "    prev_line = letters[0].line\n",
    "    print()\n",
    "    for i in range(len(letters)):\n",
    "        img = Image.fromarray(letters[i].image.astype('uint8'))\n",
    "        convert_tensor = transforms.Compose([\n",
    "            transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),\n",
    "            transforms.Grayscale(1),\n",
    "            transforms.ToTensor()\n",
    "\n",
    "        ])        \n",
    "        x_image = convert_tensor(img)\n",
    "        aaa = transforms.ToPILImage()\n",
    "        display(aaa(x_image))\n",
    "        x_image = x_image.unsqueeze(0).float()\n",
    "        x_image = x_image.to(device)\n",
    "        pred = model(x_image) \n",
    "        pred_arg_max = pred.argmax().item()\n",
    "        mapped = mnt.map_pred(pred_arg_max)\n",
    "        \n",
    "        #cv2.putText(output, mapped+' '+\"{:.2f}\".format(pred.max().item()), (letters[i].x, letters[i].y), cv2.FONT_HERSHEY_COMPLEX, 0.45, (0, 0, 0), 1)\n",
    "\n",
    "        # Draw non-ascii text onto image\n",
    "        font = ImageFont.truetype(\"ARIALUNI.TTF\", 24, encoding=\"unic\")\n",
    "        draw = ImageDraw.Draw(output)\n",
    "        draw.text((letters[i].x, letters[i].y), mapped+' '+\"{:.2f}\".format(pred.max().item()), font=font)\n",
    "        \n",
    "        x = letters[i].x\n",
    "        y = letters[i].y\n",
    "        size = (letters[i].width, letters[i].height)\n",
    "        if (letters[i].line >  prev_line):\n",
    "            s_out += \"\\n\"\n",
    "            prev_line = letters[i].line\n",
    "        prev_loc, prev_size = (x,y), size\n",
    "        s_out += mapped + ' '\n",
    "        print(letters[i].image.shape, \"{:.2f}\".format(pred.max().item()), mapped)\n",
    "    #output = Image.fromarray(output.astype('uint8'))\n",
    "    display(output)\n",
    "    return s_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38953a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION\n"
     ]
    }
   ],
   "source": [
    "model = factory.LoadModel()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print('EVALUATION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf68e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = img_to_str(model, 'TEST/5.jpg')\n",
    "# print('RESULT:')\n",
    "# print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef2b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "803\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from imutils import contours\n",
    "import pytesseract\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = \"C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe\"\n",
    "\n",
    "img = cv2.imread('TEST/eng.jpg')\n",
    "height,width,_=img.shape\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "thresh = cv2.threshold(gray,0,255,cv2.THRESH_OTSU)[1]\n",
    "\n",
    "cnts = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts,_= contours.sort_contours(cnts[0])\n",
    "print(len(cnts))\n",
    "# for c in cnts:\n",
    "#     area=cv2.contourArea(c)\n",
    "#     x,y,w,h=cv2.boundingRect(c)\n",
    "#     if area>20:\n",
    "#         img_area=img[y:y+h,x:x+h]\n",
    "#         cv2.imshow('MyPhoto', img_area)\n",
    "#         cv2.waitKey(0)\n",
    "\n",
    "#         cv2.destroyAllWindows()\n",
    "#         result=pytesseract.image_to_string(img_area,lang=\"eng\")\n",
    "#         print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c83a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abracadabra():\n",
    "    img = cv2.imread('TEST/real1.jpg')\n",
    "    cv2.imshow('MyPhoto', img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #set a thresh\n",
    "    thresh = 100\n",
    "    ret, thresh_img = cv2.threshold(gray, thresh, 255, cv2.THRESH_BINARY)\n",
    "    img_erode = cv2.erode(thresh_img, np.ones((3, 3), np.uint8), iterations=1)\n",
    "\n",
    "    # Get contours\n",
    "    contours, hierarchy = cv2.findContours(img_erode, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    cv2.imshow('contours', resize_image(thresh_img, 224))\n",
    "    #     cv2.waitKey()\n",
    "    #     cv2.destroyAllWindows()\n",
    "\n",
    "    output = img.copy()\n",
    "\n",
    "    img_contours = np.uint8(np.zeros((img.shape[0],img.shape[1])))\n",
    "    cv2.drawContours(img_contours, contours, -1, (255,255,255), 1)\n",
    "    cv2.imshow('origin', resize_image(img, 224)) # выводим итоговое изображение в окно\n",
    "    #cv2.imshow('gray', resize_image(gray, 224)) # выводим итоговое изображение в окно\n",
    "    cv2.imshow('res', resize_image(img_contours, 224)) # выводим итоговое изображение в окно\n",
    "\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80cc3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
