{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c62fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from PIL import Image, ImageOps, ImageFont, ImageDraw\n",
    "from torchvision import transforms\n",
    "\n",
    "from imutils.object_detection import non_max_suppression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "import MathNet as mnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c3631a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'models\\mathnet\\mathnet22.ml'\n",
    "NUM_CLASSES = mnt.NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98d0a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUDA_LAUNCH_BLOCKING=1\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3377f4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels=1, out_channels=6, kernel_size=5, padding=2)\n",
    "        self.act1  = torch.nn.ReLU()\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "       \n",
    "        self.conv2_1 = torch.nn.Conv2d(\n",
    "            in_channels=6, out_channels=12, kernel_size=3, padding=0)\n",
    "        self.conv2_2 = torch.nn.Conv2d(\n",
    "            in_channels=12, out_channels=16, kernel_size=3, padding=0)\n",
    "        self.act2  = torch.nn.ReLU()\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc1   = torch.nn.Linear(5 * 5 * 16, 120)\n",
    "        self.act3  = torch.nn.ReLU()\n",
    "        \n",
    "        self.fc2   = torch.nn.Linear(120, 84)\n",
    "        self.act4  = torch.nn.Tanh()\n",
    "        \n",
    "        self.fc3   = torch.nn.Linear(84, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2_1(x)\n",
    "        x = self.conv2_2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = x.view(x.size(0), x.size(1) * x.size(2) * x.size(3))\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "434ef9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Letter:\n",
    "    def __init__(self, x, y, w, h, img):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.width = w\n",
    "        self.height = h\n",
    "        self.image = img\n",
    "        \n",
    "        self.line = 0\n",
    "        \n",
    "        self.bottom = self.y + self.height\n",
    "        self.top = self.y\n",
    "        self.left = self.x\n",
    "        self.right = self.x + self.width\n",
    "        \n",
    "    def resize():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e90c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "(H, W) = (400, 400)\n",
    "\n",
    "def custom_sort(countour):\n",
    "        return -cv2.contourArea(countour)\n",
    "\n",
    "class SlidingWindowObjectDetection():\n",
    "    def __init__(self, pretrained_classifier_path, kwargs):\n",
    "        self.model = mnt.MathNet()\n",
    "        self.model.load_state_dict(torch.load(MODEL_PATH))\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = self.model.to(device)\n",
    "        self.model.eval()\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def get_rois(self, image, step, window_stride):\n",
    "        potential = []\n",
    "        for y in range(0, image.shape[0] - window_stride[1], step):\n",
    "            for x in range(0, image.shape[1] - window_stride[0], step):\n",
    "                crop_img = image[y:y + self.kwargs['INPUT_SIZE'][0], x:x + self.kwargs['INPUT_SIZE'][1]]\n",
    "                \n",
    "                thresh = 120\n",
    "                ret, thresh_img = cv2.threshold(crop_img, thresh, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "                #find contours\n",
    "                contours, hierarchy = cv2.findContours(thresh_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                img_contours = np.uint8(np.zeros((crop_img.shape[0],crop_img.shape[1])))\n",
    "                cv2.drawContours(img_contours, contours, -1, (255,255,255), 1)\n",
    "                \n",
    "                if (len(contours) > 1):\n",
    "                    #print(x,y, len(contours))\n",
    "                    img = Image.fromarray(crop_img.astype('uint8'))\n",
    "                    #display(img)\n",
    "                    letter = Letter(x, y, crop_img.shape[1], crop_img.shape[0], crop_img)\n",
    "                    potential.append(letter)\n",
    "        return potential\n",
    "    \n",
    "    def predict(self, letters):\n",
    "        res = []\n",
    "        ind = 0\n",
    "        for letter in letters:\n",
    "            img = Image.fromarray(letter.image.astype('uint8'))\n",
    "            convert_tensor = transforms.Compose([\n",
    "                transforms.Resize((224,224)),\n",
    "                transforms.Grayscale(1),\n",
    "                transforms.ToTensor()\n",
    "\n",
    "            ])\n",
    "            convert= transforms.Compose([\n",
    "                transforms.Resize((224,224)),\n",
    "                transforms.Grayscale(1)\n",
    "\n",
    "            ])\n",
    "\n",
    "            #display(convert(img))\n",
    "            x_image = convert_tensor(img)\n",
    "            x_image = x_image.unsqueeze(0).float()\n",
    "            x_image = x_image.to(device)\n",
    "\n",
    "            preds = self.model(x_image) \n",
    "            prob = preds.max().item()\n",
    "            #print(prob)\n",
    "            if prob >= self.kwargs['MIN_CONF']:\n",
    "                #print(prob, (mnt.map_pred(preds.argmax()), preds))\n",
    "                #img = Image.fromarray(letter.image.astype('uint8'))\n",
    "                display(img)\n",
    "                #cv2.waitKey(1000)\n",
    "            \n",
    "                res.append(ind)\n",
    "            ind += 1\n",
    "        return res\n",
    "        \n",
    "    def visualize_preds(self, img, letters, indices):\n",
    "        output = img.copy()\n",
    "        for ind in indices:\n",
    "            letter = letters[ind]\n",
    "            rect = cv2.rectangle(output, (letter.x, letter.y), (letter.right, letter.bottom), (0, 255,0), 2)\n",
    "       \n",
    "        aaa = Image.fromarray(output.astype('uint8'))\n",
    "        display(aaa)\n",
    "        return letters\n",
    "    \n",
    "    \n",
    "\n",
    "    def get_exact_locations(self, rois):\n",
    "        res = []\n",
    "        for letter in rois:\n",
    "            img = letter.image\n",
    "            output = img.copy()\n",
    "            gray = img\n",
    "\n",
    "            thresh = 100\n",
    "            ret, thresh_img = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    \n",
    "            img_erode = thresh_img\n",
    "\n",
    "            # Get contours\n",
    "            contours, hierarchy = cv2.findContours(img_erode, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)       \n",
    "            img_contours = np.uint8(np.zeros((img.shape[0],img.shape[1])))\n",
    "            cv2.drawContours(img_contours, contours, -1, (255,255,255), 1)\n",
    "\n",
    "            contours = list(contours)\n",
    "            contours.sort(key=custom_sort)\n",
    "            my_countour = contours[1]\n",
    "\n",
    "            (x, y, w, h) = cv2.boundingRect(my_countour)\n",
    "            #print(\"R\", x, y, w, h, cv2.contourArea(contour))\n",
    "            crop_img = img_erode[y:y+h, x:x+w]\n",
    "            size_max = max(w, h)\n",
    "            letter_square = 255 * np.ones(shape=[size_max, size_max], dtype=np.uint8)\n",
    "            if w > h:\n",
    "                y_pos = size_max//2 - h//2\n",
    "                letter_square[y_pos:y_pos + h, 0:w] = crop_img\n",
    "            elif w < h:\n",
    "                x_pos = size_max//2 - w//2\n",
    "                letter_square[0:h, x_pos:x_pos + w] = crop_img\n",
    "            else:\n",
    "                letter_square = crop_img\n",
    "            rect = cv2.rectangle(output, (x, y), (x + w, y + h), (0, 255,0), 2)\n",
    "            \n",
    "            _let = Letter(x+letter.x, y+letter.y, w, h, letter_square)\n",
    "            res.append(_let)\n",
    "\n",
    "        res.sort(key=lambda ll: (ll.y, ll.x), reverse=False)\n",
    "        aaa = Image.fromarray(output.astype('uint8'))\n",
    "        #display(aaa)\n",
    "        return res\n",
    "\n",
    "    def __call__(self, img):\n",
    "        regions_of_interest = self.get_rois(img, self.kwargs['WIN_STEP'], self.kwargs['ROI_SIZE'])\n",
    "        print(len(regions_of_interest))\n",
    "        letters = self.get_exact_locations(regions_of_interest)\n",
    "        print(len(letters))\n",
    "        indices = self.predict(letters)\n",
    "        print(len(indices))\n",
    "        #nms_labels = self.apply_nms(labels)\n",
    "        #if self.kwargs['VISUALIZE']:\n",
    "        self.visualize_preds(img, letters, indices)\n",
    "        return 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70c2aa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(\n",
    "    PYR_SCALE=1.25,\n",
    "    WIN_STEP=16,\n",
    "    ROI_SIZE=(0, 0),\n",
    "    INPUT_SIZE=(32, 32),\n",
    "    VISUALIZE=True,\n",
    "    MIN_CONF=1.45,\n",
    "    VIZ_ROIS=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74f8d7d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "40\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABMAAAATCAAAAABXO2kQAAAAT0lEQVR4nJWQMQ4AIAgDW+P/v1wHFGpwkQnPE2IptJrZEQBkjHGmeyoVGDaH+2p0ZIzVycUwfZ4UsjP0HVk348trO37/YbkchIkMqh48cl5F/REs5tu7rwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=19x19>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAMAAAADCAAAAABzQ+pjAAAAFElEQVR4nGNk+M/A9J+BgYmBgQEAER8CBHa2kjcAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=3x3>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABIAAAASCAAAAABzpdGLAAAAS0lEQVR4nI2QQQ6AMAgEZ03//+XxUE2pRCM3hmXZEHnW0UhFSUN21T+vDzRItZ4IISlwQETIymdpvNEaT1YvXgvZPhHAPZc21Vv6E8urESCMMfTfAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=18x18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAAAAAA6I3INAAAANElEQVR4nK2NMQ4AIAwCD9P/fxkHW62Dm0yQHDmZngCU3WtyiHGxP2dQYgOonMId9vO7WSYbEwgfSfRLTQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=14x14>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAAAAAA6I3INAAAANElEQVR4nK2NMQ4AIAwCD9P/fxkHW62Dm0yQHDmZngCU3WtyiHGxP2dQYgOonMId9vO7WSYbEwgfSfRLTQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=14x14>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAsAAAALCAAAAACMxyj6AAAAGElEQVR4nGP8zwAHTAyksRkQev8zUmAOAEYxBA95EnndAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=11x11>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAsAAAALCAAAAACMxyj6AAAAGElEQVR4nGP8zwAHTAyksRkQev8zUmAOAEYxBA95EnndAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=11x11>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAcAAAAHCAAAAADhOQgPAAAAIklEQVR4nGXJIRIAMAzDMGX//3MKekM1EXAKPN8E0UJz/jalMAUJA9neYgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=7x7>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAcAAAAHCAAAAADhOQgPAAAAIklEQVR4nGXJIRIAMAzDMGX//3MKekM1EXAKPN8E0UJz/jalMAUJA9neYgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=7x7>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAUAAAAFCAAAAACoBHk5AAAAHElEQVR4nE3EoQEAAAwCIPT/n10dgSyjFor34gBRlgQJ3YoOYwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=5x5>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAAAAAA6mKC9AAAARUlEQVR4nF2PQRIAIAgCWaf/f5kOlamenBVQsWrhkERFMQQHMBQtpllI8DyW8AAru4uPxSQuodQt5vmizz/I434G+U6vDbW8Dh9B6J9OAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=16x16>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAwAAAAMCAAAAABzHgM7AAAAPElEQVR4nE3OMQ7AQAgDwTG6/3+ZFFxIqGxWrEinzaSLzbZkelrQFJKLzuyQv6CHrOBsuiTv2SdA2Xd4APZNDiL9Hw9eAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=12x12>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAwAAAAMCAAAAABzHgM7AAAAPElEQVR4nE3OMQ7AQAgDwTG6/3+ZFFxIqGxWrEinzaSLzbZkelrQFJKLzuyQv6CHrOBsuiTv2SdA2Xd4APZNDiL9Hw9eAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=12x12>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAA8AAAAPCAAAAAAevcqWAAAAOklEQVR4nI2PMQ4AMAgCT+P/v0yXDjDY1O1EIJaIaSjnwRdiAJX8HkM642hA7PpwC+R9Cv3l/+D48ACLCgokSrkUHQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=15x15>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAsAAAALCAAAAACMxyj6AAAAKElEQVR4nGP8zwAHTAwMDAwMjEhsBqLZjP+hGhn+MzD+h4oQ1AuxHABTWgYR5+8Y1QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=11x11>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAsAAAALCAAAAACMxyj6AAAAKElEQVR4nGP8zwAHTAwMDAwMjEhsBqLZjP+hGhn+MzD+h4oQ1AuxHABTWgYR5+8Y1QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=11x11>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAUAAAAFCAAAAACoBHk5AAAAG0lEQVR4nAXBgQ0AAAgCIHT9/7JBBoUUdogMHi+yAwobQLg/AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=5x5>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAUAAAAFCAAAAACoBHk5AAAAG0lEQVR4nAXBgQ0AAAgCIHT9/7JBBoUUdogMHi+yAwobQLg/AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=5x5>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAYAAAAGCAAAAADFp7CUAAAAHklEQVR4nC3HsQ0AAAgDIOr/P9dBJ0IKhhzC0N/TWD8iAwy2REofAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=6x6>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAYAAAAGCAAAAADFp7CUAAAAHklEQVR4nC3HsQ0AAAgDIOr/P9dBJ0IKhhzC0N/TWD8iAwy2REofAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=6x6>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAQAAAAECAAAAACMmsGiAAAAGUlEQVR4nAXBAQEAAAjDIHz/zhOkpjMY3QNDGgUEc2VoiwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=4x4>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAQAAAAECAAAAACMmsGiAAAAGUlEQVR4nAXBAQEAAAjDIHz/zhOkpjMY3QNDGgUEc2VoiwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=4x4>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAQAAAAECAAAAACMmsGiAAAAGUlEQVR4nAXBAQEAAAjDIHz/zhOkpjMY3QNDGgUEc2VoiwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=4x4>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAkAAAAJCAAAAADF+lnMAAAAJUlEQVR4nGP8zwAFTAwYLEYWKM3wn4WBkYGBgYHhPwMjPh1ILADg/QQUwUM71wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=9x9>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAkAAAAJCAAAAADF+lnMAAAAJUlEQVR4nGP8zwAFTAwYLEYWKM3wn4WBkYGBgYHhPwMjPh1ILADg/QQUwUM71wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=9x9>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=468x370>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END\n"
     ]
    }
   ],
   "source": [
    "sw = SlidingWindowObjectDetection(MODEL_PATH, kwargs)\n",
    "IMAGE_NAME = 'TEST//5.jpg'\n",
    "image = cv2.imread(IMAGE_NAME)\n",
    "img_grey = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "sw(img_grey)\n",
    "print('END')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b78ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "344ea2c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m stepSize \u001b[38;5;241m=\u001b[39m (winW \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     28\u001b[0m cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x, y, window) \u001b[38;5;129;01min\u001b[39;00m sliding_window(image, stepSize\u001b[38;5;241m=\u001b[39mstepSize, windowSize\u001b[38;5;241m=\u001b[39m(winW, winH)):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# if the window does not meet our desired window size, ignore it\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m window\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m winH \u001b[38;5;129;01mor\u001b[39;00m window\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m winW:\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[49], line 3\u001b[0m, in \u001b[0;36msliding_window\u001b[1;34m(image, stepSize, windowSize)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msliding_window\u001b[39m(image, stepSize, windowSize):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# slide a window across the image\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstepSize\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], stepSize):\n\u001b[0;32m      5\u001b[0m             \u001b[38;5;66;03m# yield the current window\u001b[39;00m\n\u001b[0;32m      6\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m (x, y, image[y:y \u001b[38;5;241m+\u001b[39m windowSize[\u001b[38;5;241m1\u001b[39m], x:x \u001b[38;5;241m+\u001b[39m windowSize[\u001b[38;5;241m0\u001b[39m]])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "def sliding_window(image, stepSize, windowSize):\n",
    "    # slide a window across the image\n",
    "    for y in range(0, image.shape[0], stepSize):\n",
    "        for x in range(0, image.shape[1], stepSize):\n",
    "            # yield the current window\n",
    "            yield (x, y, image[y:y + windowSize[1], x:x + windowSize[0]])\n",
    "\n",
    "\n",
    "# Возврат в набор результатов скользящего окна, временно не использованный\n",
    "def get_slice(image, stepSize, windowSize):\n",
    "    slice_sets = []\n",
    "    for (x, y, window) in sliding_window(image, stepSize, windowSize):\n",
    "        # if the window does not meet our desired window size, ignore it\n",
    "        if window.shape[0] != winH or window.shape[1] != winW:\n",
    "            continue\n",
    "        slice = image[y:y + winH, x:x + winW]\n",
    "        slice_sets.append(slice)\n",
    "    return slice_sets\n",
    "\n",
    "\n",
    "\n",
    "image = cv2.imread('TEST//5.jpg')\n",
    "\n",
    "# Настройка размера скользящего окна\n",
    "(winW, winH) = (32,32)\n",
    "# Размер шага\n",
    "stepSize = (winW / 2)\n",
    "cnt = 0\n",
    "for (x, y, window) in sliding_window(image, stepSize=stepSize, windowSize=(winW, winH)):\n",
    "    # if the window does not meet our desired window size, ignore it\n",
    "    if window.shape[0] != winH or window.shape[1] != winW:\n",
    "        continue\n",
    "    # since we do not have a classifier, we'll just draw the window\n",
    "    clone = image.copy()\n",
    "    cv2.rectangle(clone, (x, y), (x + winW, y + winH), (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Window\", clone)\n",
    "    cv2.waitKey(1000)\n",
    "\n",
    "    slc = image[y:y+winH,x:x+winW]\n",
    "    cv2.namedWindow('sliding_slice',0)\n",
    "    cv2.imshow('sliding_slice', slc)\n",
    "    cv2.waitKey(1000)\n",
    "    cnt = cnt + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ad41a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d11bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2413d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfcf41c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f789fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, dst):\n",
    "#     # соотношение сторон: ширина, делённая на ширину оригинала\n",
    "#     ratio = float(dwt / image.shape[1])\n",
    "#     print(ratio)\n",
    "#     # желаемая высота: высота, умноженная на соотношение сторон\n",
    "#     dht = int(image.shape[0] * ratio)\n",
    "#     print(image.shape[0] * ratio)\n",
    "#     dim = (dwt, dht)  # итоговые размеры\n",
    "#     print(dim)\n",
    "    # Масштабируем картинку\n",
    "    # Подготовим новые размеры\n",
    "    x = float(dst) / image.shape[1]\n",
    "    y = float(dst) / image.shape[0]\n",
    "    # уменьшаем изображение до подготовленных размеров\n",
    "    return cv2.resize(image, (0,0), fx=x,fy=y, interpolation = cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af995677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_letter(image, dst):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3e62ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Letter:\n",
    "    def __init__(self, x, y, w, h, img):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.width = w\n",
    "        self.height = h\n",
    "        self.image = img\n",
    "        \n",
    "        self.line = 0\n",
    "        \n",
    "        self.bottom = self.y + self.height\n",
    "        self.top = self.y\n",
    "        self.left = self.x\n",
    "        self.right = self.x + self.width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0d158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letters_extract(image_file, out_size=224):\n",
    "    img = cv2.imread(image_file)\n",
    "    output = img.copy()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #set a thresh\n",
    "    thresh = 100\n",
    "    ret, thresh_img = cv2.threshold(gray, thresh, 255, cv2.THRESH_BINARY)\n",
    "    img_erode = cv2.erode(thresh_img, np.ones((3, 3), np.uint8), iterations=3)\n",
    "#     cv2.imshow('MyPhoto', img_erode)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "    \n",
    "\n",
    "    # Get contours\n",
    "    contours, hierarchy = cv2.findContours(img_erode, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)       \n",
    "    img_contours = np.uint8(np.zeros((img.shape[0],img.shape[1])))\n",
    "    cv2.drawContours(img_contours, contours, -1, (255,255,255), 1)\n",
    "#     cv2.imshow('MyPhoto', img_contours )\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "       \n",
    "    # Filter contours\n",
    "    mask = np.uint8(np.zeros((img.shape[0],img.shape[1])))\n",
    "    for idx, contour in enumerate(contours):\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        if cv2.contourArea(contour) > 100 and cv2.contourArea(contour) < 10000:\n",
    "            cv2.drawContours(mask, [contour], 0, (255), -1)\n",
    "        else:\n",
    "            pass\n",
    "#             dummy = np.uint8(np.zeros((img.shape[0],img.shape[1])))\n",
    "#             cv2.drawContours(dummy, contour, -1, (255,255,255), 1)\n",
    "#             print(w*h, cv2.contourArea(contour))\n",
    "#             cv2.imshow(str(idx), dummy )\n",
    "#             cv2.waitKey(0)\n",
    "#             cv2.destroyAllWindows()\n",
    "            \n",
    "#     # apply the mask to the original image\n",
    "    result = cv2.bitwise_and(img,img, mask= mask)   \n",
    "#     cv2.imshow('result', result )\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "    \n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    img_contours = np.uint8(np.zeros((img.shape[0],img.shape[1])))\n",
    "    cv2.drawContours(img_contours, contours, -1, (255,255,255), 1)\n",
    "#     cv2.imshow('MyPhoto', img_contours )\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "    \n",
    "    letters = []\n",
    "    for idx, contour in enumerate(contours):\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        #print(\"R\", idx, x, y, w, h, cv2.contourArea(contour), hierarchy[0][idx])\n",
    "        if hierarchy[0][idx][3] != -1:\n",
    "            continue\n",
    "#         mask_contour = img_contours[y:y+h, x:x+w]\n",
    "#         mask_contour[y_pos:y_pos + h, 0:w] = crop_img\n",
    "#         cv2.drawContours(mask_contour, contour, -1, (255,255,255), 1)\n",
    "#         cv2.imshow('MyPhoto', mask_contour)\n",
    "#         cv2.waitKey(0)\n",
    "#         cv2.destroyAllWindows()\n",
    "\n",
    "#         cv2.imwrite('mask.png', img * max(img - 100, 0) * 255)\n",
    "#         ii = cv2.imread('mask.png')\n",
    "#         cv2.imshow('MyPhoto', ii )\n",
    "#         cv2.waitKey(0)\n",
    "#         cv2.destroyAllWindows()\n",
    "        crop_img = img_erode[y:y+h, x:x+w]\n",
    "        #crop_img = thresh_img[y:y+h, x:x+w]\n",
    "        size_max = max(w, h)\n",
    "        letter_square = 255 * np.ones(shape=[size_max, size_max], dtype=np.uint8)\n",
    "        if w > h:\n",
    "            y_pos = size_max//2 - h//2\n",
    "            letter_square[y_pos:y_pos + h, 0:w] = crop_img\n",
    "        elif w < h:\n",
    "            x_pos = size_max//2 - w//2\n",
    "            letter_square[0:h, x_pos:x_pos + w] = crop_img\n",
    "        else:\n",
    "            letter_square = crop_img\n",
    "        \n",
    "        x,y,w,h = cv2.boundingRect(contour)\n",
    "        rect = cv2.rectangle(output, (x, y), (x + w, y + h), (0, 255,0), 2)      \n",
    "        inverted = cv2.bitwise_not(letter_square)\n",
    "        \n",
    "        letter = Letter(x,y,w,h,letter_square)\n",
    "        letters.append(letter)\n",
    "\n",
    "    letters.sort(key=lambda ll: (ll.y, ll.x), reverse=False)\n",
    "    return letters, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2025e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_size(lst):\n",
    "    if len(lst) <= 0:\n",
    "        return (0, 0)\n",
    "    avg_w = 0\n",
    "    avg_h = 0\n",
    "    for letter in lst:\n",
    "        avg_w += letter.width\n",
    "        avg_h += letter.height\n",
    "    avg_w /= len(lst)\n",
    "    avg_h /= len(lst)\n",
    "    \n",
    "    return (avg_w, avg_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf06bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_str(model, image_file):\n",
    "    letters, output = letters_extract(image_file)   \n",
    "    output = Image.fromarray(output.astype('uint8'))\n",
    "    print('SHAPE: ', np.array(letters,dtype=object).shape)\n",
    "    s_out = \"\"\n",
    "    if len(letters) == 0:\n",
    "        return \"Found nothing\"\n",
    "    (avg_w, avg_h) = average_size(letters)\n",
    "    print((avg_w, avg_h))\n",
    "    \n",
    "    # True sorting by Y axis\n",
    "    line = 0\n",
    "    for i in range (1, len(letters)):\n",
    "        if letters[i].top > letters[i-1].bottom:\n",
    "            line += 1\n",
    "        letters[i].line = line\n",
    "    letters.sort(key=lambda ll: (ll.line, ll.x), reverse=False)  \n",
    "    \n",
    "    prev_loc = (letters[0].x, letters[0].y)\n",
    "    prev_size = (letters[0].width, letters[0].height)\n",
    "    prev_line = letters[0].line\n",
    "    print()\n",
    "    for i in range(len(letters)):\n",
    "        img = Image.fromarray(letters[i].image.astype('uint8'))\n",
    "        convert_tensor = transforms.Compose([\n",
    "            transforms.Resize((28,28)),\n",
    "            transforms.Grayscale(1),\n",
    "            transforms.ToTensor()\n",
    "\n",
    "        ])        \n",
    "        x_image = convert_tensor(img)\n",
    "        aaa = transforms.ToPILImage()\n",
    "        display(aaa(x_image))\n",
    "        x_image = x_image.unsqueeze(0).float()\n",
    "        x_image = x_image.to(device)\n",
    "        pred = model(x_image) \n",
    "        pred_arg_max = pred.argmax().item()\n",
    "        mapped = map_pred(pred_arg_max)\n",
    "        \n",
    "        #cv2.putText(output, mapped+' '+\"{:.2f}\".format(pred.max().item()), (letters[i].x, letters[i].y), cv2.FONT_HERSHEY_COMPLEX, 0.45, (0, 0, 0), 1)\n",
    "\n",
    "        # Draw non-ascii text onto image\n",
    "        font = ImageFont.truetype(\"ARIALUNI.TTF\", 24, encoding=\"unic\")\n",
    "        draw = ImageDraw.Draw(output)\n",
    "        draw.text((letters[i].x, letters[i].y), mapped+' '+\"{:.2f}\".format(pred.max().item()), font=font)\n",
    "        \n",
    "        x = letters[i].x\n",
    "        y = letters[i].y\n",
    "        size = (letters[i].width, letters[i].height)\n",
    "        if (letters[i].line >  prev_line):\n",
    "            s_out += \"\\n\"\n",
    "            prev_line = letters[i].line\n",
    "        prev_loc, prev_size = (x,y), size\n",
    "        s_out += mapped + ' '\n",
    "        print(letters[i].image.shape, \"{:.2f}\".format(pred.max().item()), mapped)\n",
    "    #output = Image.fromarray(output.astype('uint8'))\n",
    "    display(output)\n",
    "    return s_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38953a63",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MathNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMathNet\u001b[49m()\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(MODEL_PATH))\n\u001b[0;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MathNet' is not defined"
     ]
    }
   ],
   "source": [
    "model = MathNet()\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print('EVALUATION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf68e22",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = img_to_str(model, '5.jpg')\n",
    "print('RESULT:')\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef2b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c83a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abracadabra():\n",
    "    img = cv2.imread('real.jpg')\n",
    "    cv2.imshow('MyPhoto', img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #set a thresh\n",
    "    thresh = 100\n",
    "    ret, thresh_img = cv2.threshold(gray, thresh, 255, cv2.THRESH_BINARY)\n",
    "    img_erode = cv2.erode(thresh_img, np.ones((3, 3), np.uint8), iterations=1)\n",
    "\n",
    "    # Get contours\n",
    "    contours, hierarchy = cv2.findContours(img_erode, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    cv2.imshow('contours', resize_image(thresh_img, 224))\n",
    "    #     cv2.waitKey()\n",
    "    #     cv2.destroyAllWindows()\n",
    "\n",
    "    output = img.copy()\n",
    "\n",
    "    img_contours = np.uint8(np.zeros((img.shape[0],img.shape[1])))\n",
    "    cv2.drawContours(img_contours, contours, -1, (255,255,255), 1)\n",
    "    cv2.imshow('origin', resize_image(img, 224)) # выводим итоговое изображение в окно\n",
    "    #cv2.imshow('gray', resize_image(gray, 224)) # выводим итоговое изображение в окно\n",
    "    cv2.imshow('res', resize_image(img_contours, 224)) # выводим итоговое изображение в окно\n",
    "\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80cc3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3262b13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MathNet()\n",
    "model.load_state_dict(torch.load('models//mathnet/mathnet40.ml'))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "img = Image.open(\"0.jpg\")\n",
    "#print(img.shape)\n",
    "convert_tensor = transforms.Compose([\n",
    "    transforms.Resize((28,28)),\n",
    "    transforms.Grayscale(1),\n",
    "    transforms.ToTensor()\n",
    "\n",
    "])        \n",
    "x_image = convert_tensor(img)\n",
    "\n",
    "aaa = transforms.ToPILImage()\n",
    "display(aaa(x_image))\n",
    "\n",
    "x_image = x_image.unsqueeze(0).float()\n",
    "x_image = x_image.to(device)\n",
    "\n",
    "pred = model(x_image) \n",
    "print(pred*10, map_pred(pred.argmax().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4af59fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a15b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28961b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a9881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7603e35a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
