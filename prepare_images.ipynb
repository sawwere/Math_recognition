{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1f48c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import models.MathNet as mnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2360c1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 160\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "435dfc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf58412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training transforms\n",
    "train_transform = transforms.Compose([\n",
    "    #transforms.CenterCrop((150, 150)),\n",
    "    transforms.Resize(desired_size),\n",
    "\n",
    "    transforms.Grayscale(1)\n",
    "    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2df351f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=224x224>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = torchvision.datasets.ImageFolder('T:\\my_programs\\Math_recognition\\TRAIN\\dataset_balance224', transform=train_transform)\n",
    "display(dataset[112][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "426fabf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_dict = dataset.class_to_idx\n",
    "classes = []\n",
    "for key in classes_dict.keys():\n",
    "    classes.append(str(key))\n",
    "classes.sort()\n",
    "#classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67725534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classes = ['!', '(', ')', '+', ',', '-', \n",
    "#            '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', \n",
    "#            'A', 'C', 'Delta', 'G', 'H', 'M', 'N', 'R', 'S', 'T', 'X', '[', ']', \n",
    "#            'alpha', 'ascii_124', 'b', 'beta', 'chi', 'cos', 'd', 'div', 'e', 'exists', \n",
    "#            'f', 'forall', 'forward_slash', 'gamma', 'geq', 'greater', 'gt', \n",
    "#            'i', 'in', 'infty', 'int', 'j', 'k', 'l', 'lambda', 'ldots', 'leq', \n",
    "#            'lim', 'log', 'lt', 'mu', 'neq', 'o', 'omega', 'p', 'phi', 'pi', 'pm', 'prime', \n",
    "#            'q', 'rightarrow', 'sigma', 'sin', 'sqrt', 'sum', 'tan', 'theta', 'times', \n",
    "#            'u', 'upsilon', 'v', 'w', 'y', 'z', '{', '}']\n",
    "\n",
    "NUM_CLASSES = len(classes)\n",
    "\n",
    "def map_pred(ind):\n",
    "    if ind < NUM_CLASSES:\n",
    "        return classes[ind]\n",
    "    return 'ERROR MAPPIMG'\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97dc7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=6, pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d5d837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a26e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize():\n",
    "    path = \"G:/TRAIN/images_resized_\"+str(desired_size[0])+\"/\"\n",
    "    counter = 0\n",
    "    for p in tqdm(dataset, total=int(len(dataset))):\n",
    "        true_name = map_pred(p[1])\n",
    "        Path(path + true_name).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        #image = np.array(p[0])\n",
    "        #gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        #thresh = 250\n",
    "        #ret, thresh_img = cv2.threshold(image, thresh, 255, cv2.THRESH_BINARY)\n",
    "        #img_erode = cv2.erode(thresh_img, np.ones((3, 3), np.uint8), iterations=1)\n",
    "        \n",
    "        #cv2.imwrite(path + true_name + '/' + str(counter)+'.jpg', img_erode)\n",
    "        #rotated.save(path + true_name + '/' + str(counter)+'.jpg')\n",
    "        train_transform(p[0]).save(path + true_name +'/' + str(counter)+'.jpg')\n",
    "        counter+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4be8f090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134442/134442 [02:35<00:00, 862.54it/s] \n"
     ]
    }
   ],
   "source": [
    "resize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f4f491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy():\n",
    "    counter = 0\n",
    "    true_name = ''\n",
    "    classes = []\n",
    "    for p in dataset:\n",
    "        #print(p)\n",
    "        true_name1 = str(p[1])#map_pred(p[1])\n",
    "        if true_name != true_name1:\n",
    "            true_name = true_name1\n",
    "            classes.append(true_name)\n",
    "            Path(\"dummy\"+str(desired_size[0])+\"/\"+true_name).mkdir(parents=True, exist_ok=True)\n",
    "            p[0].save(\"dummy\"+str(desired_size[0])+\"/\" + true_name + '/' + str(counter)+'.jpg')\n",
    "            \n",
    "        if counter % 1000 == 0:\n",
    "            print(len(dataset)-counter)\n",
    "        #Path(\"new/\"+str(p[1])).mkdir(parents=True, exist_ok=True)\n",
    "        #torch.save(p[0], 'new/'+str(p[1]) + str(counter)+'.jpg')\n",
    "        counter+=1\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2dff414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_contrast(x, factor):\n",
    "    return transforms.functional.adjust_contrast(x, factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed63abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast():\n",
    "    path = \"G:/TRAIN/images_contrast_to\"+str(desired_size[0])+\"/\"\n",
    "    counter = 0\n",
    "    for p in tqdm(dataset, total=int(len(dataset))):\n",
    "        true_name = map_pred(p[1])\n",
    "        Path(path + true_name).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        #display(p[0])\n",
    "        image = add_contrast(p[0], 3)\n",
    "        \n",
    "        #display(image)\n",
    "        \n",
    "        image.save(path + true_name + '/' + str(counter)+'.jpg')\n",
    "#         Path(\"new/\"+str(p[1])).mkdir(parents=True, exist_ok=True)\n",
    "#         torch.save(p[0], 'new/'+str(p[1]) + str(counter)+'.jpg')\n",
    "        counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "797d3267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134735/134735 [03:31<00:00, 638.33it/s]\n"
     ]
    }
   ],
   "source": [
    "contrast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d000b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1dbce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be31384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa686fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', ')', ',', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'F', 'X', 'Y', 'Z', 'alpha', 'and', 'beta', 'delta', 'exists', 'forall', 'gamma', 'lambda', 'mu', 'not', 'omega', 'or', 'phi', 'pi', 'psi', 'rightarrow', 'sigma', 'tau', 'theta', 'upsilon']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root, dirs, files = os.walk(\"F:\\my_programs\\python\\image_recognition\\\\all_dataset\").__next__()\n",
    "\n",
    "dirs.sort()\n",
    "print(dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f1ec99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd3828b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afe09c32",
   "metadata": {},
   "source": [
    "EMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8780231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as T\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Создается класс для чтения dataset из csv файла \n",
    "class CustomDatasetEMNIST():\n",
    "    def __init__(self, csv_file_path, \n",
    "                 transform=None,\n",
    "                 nrows=None,\n",
    "                 skiprows=None) :\n",
    "        '''nrows - кол-во строк которое необходимо подгрузить,\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.data, self.targets, self.dataframe = self._load_data(csv_file_path=csv_file_path, nrows=nrows, skiprows=skiprows)\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self) :\n",
    "        return len(self.data) \n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "       \n",
    "        img = self.data[index]\n",
    "        #print(f'1. {img.shape=} {img}/n') \n",
    "        img = img.unsqueeze(0)\n",
    "        #print(f'2.{img.shape=} {img}/n') \n",
    "        targets = int(self.targets[index])\n",
    "        # Повернем и отразим изображение для лучшего восприятия\n",
    "        img = torchvision.transforms.functional.rotate(img=img, angle=-90)\n",
    "        img = torch.flip(img, dims=(2,))\n",
    "        \n",
    "        #print(f'3 {img.shape} {img} /n') \n",
    "        \n",
    "        if self.transform is not None:\n",
    "            #print(f'3. {img.shape} {img}') \n",
    "\n",
    "            img = self.transform(img)\n",
    "            #print(f'4.{img.shape} {img}') \n",
    "\n",
    "            \n",
    "        return img, targets\n",
    "    \n",
    "    \n",
    "    def _load_data(self,\n",
    "                  csv_file_path,\n",
    "                  nrows,\n",
    "                  skiprows=None):\n",
    "        \n",
    "        df = pd.read_csv(csv_file_path, nrows=nrows, skiprows=skiprows) \n",
    "        \n",
    "        targets = torch.tensor(df.iloc[:,0].values)\n",
    "        \n",
    "        data = df.iloc[:,1:].values.reshape((len(df), 28, 28))\n",
    "        \n",
    "        return torch.tensor(data).float(), targets, df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fd33967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зададим трансформацию для трейн и тест датасетов\n",
    "train_transform = T.Compose(\n",
    "    [\n",
    "        T.Resize([224, 224]),\n",
    "        T.RandomRotation(degrees=10),\n",
    "        T.Grayscale(1),\n",
    "        #transforms.ToPILImage()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60f92ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = CustomDatasetEMNIST(csv_file_path='E:\\TRAIN\\emnist/emnist-byclass-train.csv', \n",
    "                                    \n",
    "#                                     skiprows=50000,\n",
    "                                    nrows=1000, \n",
    "                                    transform=train_transform) \n",
    "\n",
    "valid_dataset = CustomDatasetEMNIST(csv_file_path='E:\\TRAIN\\emnist/emnist-byclass-test.csv',\n",
    "#                                    nrows=100000, \n",
    "                                    transform=train_transform) \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1500, shuffle=False, num_workers=6, pin_memory=True) \n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1500, shuffle=False, num_workers=6, pin_memory=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6cfcc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def resize():\n",
    "    path = \"E:\\TRAIN\\emnist2\\\\\"\n",
    "    counter = 0\n",
    "    for p in tqdm(train_dataset, total=int(len(train_dataset))):\n",
    "        Path(path + str(p[1])).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        image = p[0].numpy().transpose(1, 2, 0)\n",
    "        #image = cv2.bitwise_not(image)\n",
    "        image = 255 - image\n",
    "        cv2.imwrite(path + str(p[1]) +'/' + str(counter)+'.jpg', image)\n",
    "        #rotated.save(path + true_name + '/' + str(counter)+'.jpg')\n",
    "        #print(train_transform(p[0]))\n",
    "        train_transform(p[0])\n",
    "        counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee6a3c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]e:\\Programs\\Anaconda3\\envs\\MyPyTorch\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 123.30it/s]\n"
     ]
    }
   ],
   "source": [
    "resize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce943e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce15509d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ff07cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067aa4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c7c80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MathNet, self).__init__()\n",
    "        self.dropout_percentage = 0.25\n",
    "        \n",
    "        #Block 1     in: 28x28x1\n",
    "        #Block 1    out: 14x14x64\n",
    "        self.conv1_1 = torch.nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1_1 = torch.nn.BatchNorm2d(num_features=64)\n",
    "        self.conv1_2 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1_2 = torch.nn.BatchNorm2d(num_features=64)\n",
    "        self.conv1_3 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1_3 = torch.nn.BatchNorm2d(num_features=64)\n",
    "        self.act1  = torch.nn.ReLU()\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        self.dropout1 = nn.Dropout(p=self.dropout_percentage)\n",
    "        \n",
    "        #Block 2     in: 14x14x64\n",
    "        #Block 2    out: 7x7x128\n",
    "        self.conv2_1 = torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2_1 = torch.nn.BatchNorm2d(num_features=128)\n",
    "        self.conv2_2 = torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2_2 = torch.nn.BatchNorm2d(num_features=128)\n",
    "        self.conv2_3 = torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2_3 = torch.nn.BatchNorm2d(num_features=128)\n",
    "        self.act2  = torch.nn.ReLU()\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        self.dropout2 = nn.Dropout(p=self.dropout_percentage)\n",
    "        \n",
    "        #Block 2     in: 7x7x128\n",
    "        #Block 2    out: 1x1x256\n",
    "        self.conv3_1 = torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn3_1 = torch.nn.BatchNorm2d(num_features=256)\n",
    "        self.conv3_2 = torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn3_2 = torch.nn.BatchNorm2d(num_features=256)\n",
    "        self.conv3_3 = torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn3_3 = torch.nn.BatchNorm2d(num_features=256)\n",
    "        self.act3  = torch.nn.ReLU()\n",
    "        self.pool3 = torch.nn.AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
    "        self.dropout3 = nn.Dropout(p=self.dropout_percentage)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(256, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1_1(x)\n",
    "        x = self.bn1_1(x)\n",
    "        x = self.conv1_2(x)\n",
    "        x = self.bn1_2(x)\n",
    "        x = self.conv1_3(x)\n",
    "        x = self.bn1_3(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2_1(x)\n",
    "        x = self.bn2_1(x)\n",
    "        x = self.conv2_2(x)\n",
    "        x = self.bn2_2(x)\n",
    "        x = self.conv2_3(x)\n",
    "        x = self.bn2_3(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3_1(x)\n",
    "        x = self.bn3_1(x)\n",
    "        x = self.conv3_2(x)\n",
    "        x = self.bn3_2(x)\n",
    "        x = self.conv3_3(x)\n",
    "        x = self.bn3_3(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = x.view(x.size(0), x.size(1) * x.size(2) * x.size(3))\n",
    "        x = self.fc1(x)  \n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
