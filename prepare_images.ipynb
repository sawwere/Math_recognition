{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1f48c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import models.MathNet as mnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2360c1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCHS = 32\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "435dfc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf58412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.CenterCrop((150, 150)),\n",
    "    transforms.Resize(desired_size),\n",
    "\n",
    "    transforms.Grayscale(1)\n",
    "    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2df351f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=224x224>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = torchvision.datasets.ImageFolder('T:\\my_programs\\Math_recognition\\TRAIN\\\\t', transform=train_transform)\n",
    "display(dataset[112][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "426fabf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_dict = dataset.class_to_idx\n",
    "classes = []\n",
    "for key in classes_dict.keys():\n",
    "    classes.append(str(key))\n",
    "classes.sort()\n",
    "#classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67725534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classes = ['!', '(', ')', '+', ',', '-', \n",
    "#            '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', \n",
    "#            'A', 'C', 'Delta', 'G', 'H', 'M', 'N', 'R', 'S', 'T', 'X', '[', ']', \n",
    "#            'alpha', 'ascii_124', 'b', 'beta', 'chi', 'cos', 'd', 'div', 'e', 'exists', \n",
    "#            'f', 'forall', 'forward_slash', 'gamma', 'geq', 'greater', 'gt', \n",
    "#            'i', 'in', 'infty', 'int', 'j', 'k', 'l', 'lambda', 'ldots', 'leq', \n",
    "#            'lim', 'log', 'lt', 'mu', 'neq', 'o', 'omega', 'p', 'phi', 'pi', 'pm', 'prime', \n",
    "#            'q', 'rightarrow', 'sigma', 'sin', 'sqrt', 'sum', 'tan', 'theta', 'times', \n",
    "#            'u', 'upsilon', 'v', 'w', 'y', 'z', '{', '}']\n",
    "\n",
    "NUM_CLASSES = len(classes)\n",
    "\n",
    "def map_pred(ind):\n",
    "    if ind < NUM_CLASSES:\n",
    "        return classes[ind]\n",
    "    return 'ERROR MAPPIMG'\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97dc7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=6, pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d5d837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29a26e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize():\n",
    "    path = \"TRAIN/images_resized_\"+str(desired_size[0])+\"/\"\n",
    "    counter = 0\n",
    "    for p in tqdm(dataset, total=int(len(dataset))):\n",
    "        true_name = map_pred(p[1])\n",
    "        Path(path + true_name).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        #image = np.array(p[0])\n",
    "        #gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        #thresh = 250\n",
    "        #ret, thresh_img = cv2.threshold(image, thresh, 255, cv2.THRESH_BINARY)\n",
    "        #img_erode = cv2.erode(thresh_img, np.ones((3, 3), np.uint8), iterations=1)\n",
    "        \n",
    "        #cv2.imwrite(path + true_name + '/' + str(counter)+'.jpg', img_erode)\n",
    "        #rotated.save(path + true_name + '/' + str(counter)+'.jpg')\n",
    "        train_transform(p[0]).save(path + true_name +'/' + str(counter)+'.jpg')\n",
    "        counter+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4be8f090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3144/3144 [00:17<00:00, 176.37it/s]\n"
     ]
    }
   ],
   "source": [
    "resize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f4f491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy():\n",
    "    counter = 0\n",
    "    true_name = ''\n",
    "    classes = []\n",
    "    for p in dataset:\n",
    "        #print(p)\n",
    "        true_name1 = str(p[1])#map_pred(p[1])\n",
    "        if true_name != true_name1:\n",
    "            true_name = true_name1\n",
    "            classes.append(true_name)\n",
    "            Path(\"dummy\"+str(desired_size[0])+\"/\"+true_name).mkdir(parents=True, exist_ok=True)\n",
    "            p[0].save(\"dummy\"+str(desired_size[0])+\"/\" + true_name + '/' + str(counter)+'.jpg')\n",
    "            \n",
    "        if counter % 1000 == 0:\n",
    "            print(len(dataset)-counter)\n",
    "        #Path(\"new/\"+str(p[1])).mkdir(parents=True, exist_ok=True)\n",
    "        #torch.save(p[0], 'new/'+str(p[1]) + str(counter)+'.jpg')\n",
    "        counter+=1\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2dff414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_contrast(x, factor):\n",
    "    return transforms.functional.adjust_contrast(x, factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed63abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast():\n",
    "    path = \"images_contrast_to\"+str(desired_size[0])+\"/\"\n",
    "    counter = 0\n",
    "    for p in tqdm(dataset, total=int(len(dataset))):\n",
    "        true_name = map_pred(p[1])\n",
    "        Path(path + true_name).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        #display(p[0])\n",
    "        image = add_contrast(p[0], 3)\n",
    "        \n",
    "        #display(image)\n",
    "        \n",
    "        image.save(path + true_name + '/' + str(counter)+'.jpg')\n",
    "#         Path(\"new/\"+str(p[1])).mkdir(parents=True, exist_ok=True)\n",
    "#         torch.save(p[0], 'new/'+str(p[1]) + str(counter)+'.jpg')\n",
    "        counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "797d3267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 173/173 [00:01<00:00, 139.67it/s]\n"
     ]
    }
   ],
   "source": [
    "contrast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d000b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1dbce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be31384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa686fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', ')', ',', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'F', 'X', 'Y', 'Z', 'alpha', 'and', 'beta', 'delta', 'exists', 'forall', 'gamma', 'lambda', 'mu', 'not', 'omega', 'or', 'phi', 'pi', 'psi', 'rightarrow', 'sigma', 'tau', 'theta', 'upsilon']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root, dirs, files = os.walk(\"F:\\my_programs\\python\\image_recognition\\\\all_dataset\").__next__()\n",
    "\n",
    "dirs.sort()\n",
    "print(dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f1ec99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd3828b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afe09c32",
   "metadata": {},
   "source": [
    "EMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8780231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as T\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Создается класс для чтения dataset из csv файла \n",
    "class CustomDatasetEMNIST():\n",
    "    def __init__(self, csv_file_path, \n",
    "                 transform=None,\n",
    "                 nrows=None,\n",
    "                 skiprows=None) :\n",
    "        '''nrows - кол-во строк которое необходимо подгрузить,\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.data, self.targets, self.dataframe = self._load_data(csv_file_path=csv_file_path, nrows=nrows, skiprows=skiprows)\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self) :\n",
    "        return len(self.data) \n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "       \n",
    "        img = self.data[index]\n",
    "        #print(f'1. {img.shape=} {img}/n') \n",
    "        img = img.unsqueeze(0)\n",
    "        #print(f'2.{img.shape=} {img}/n') \n",
    "        targets = int(self.targets[index])\n",
    "        # Повернем и отразим изображение для лучшего восприятия\n",
    "        img = torchvision.transforms.functional.rotate(img=img, angle=-90)\n",
    "        img = torch.flip(img, dims=(2,))\n",
    "        \n",
    "        #print(f'3 {img.shape} {img} /n') \n",
    "        \n",
    "        if self.transform is not None:\n",
    "            #print(f'3. {img.shape} {img}') \n",
    "\n",
    "            img = self.transform(img)\n",
    "            #print(f'4.{img.shape} {img}') \n",
    "\n",
    "            \n",
    "        return img, targets\n",
    "    \n",
    "    \n",
    "    def _load_data(self,\n",
    "                  csv_file_path,\n",
    "                  nrows,\n",
    "                  skiprows=None):\n",
    "        \n",
    "        df = pd.read_csv(csv_file_path, nrows=nrows, skiprows=skiprows) \n",
    "        \n",
    "        targets = torch.tensor(df.iloc[:,0].values)\n",
    "        \n",
    "        data = df.iloc[:,1:].values.reshape((len(df), 28, 28))\n",
    "        \n",
    "        return torch.tensor(data).float(), targets, df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fd33967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зададим трансформацию для трейн и тест датасетов\n",
    "train_transform = T.Compose(\n",
    "    [\n",
    "        T.Resize([224, 224]),\n",
    "        T.RandomRotation(degrees=10),\n",
    "        T.Grayscale(1),\n",
    "        #transforms.ToPILImage()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60f92ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = CustomDatasetEMNIST(csv_file_path='E:\\TRAIN\\emnist/emnist-byclass-train.csv', \n",
    "                                    \n",
    "#                                     skiprows=50000,\n",
    "                                    nrows=1000, \n",
    "                                    transform=train_transform) \n",
    "\n",
    "valid_dataset = CustomDatasetEMNIST(csv_file_path='E:\\TRAIN\\emnist/emnist-byclass-test.csv',\n",
    "#                                    nrows=100000, \n",
    "                                    transform=train_transform) \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1500, shuffle=False, num_workers=6, pin_memory=True) \n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1500, shuffle=False, num_workers=6, pin_memory=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6cfcc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def resize():\n",
    "    path = \"E:\\TRAIN\\emnist2\\\\\"\n",
    "    counter = 0\n",
    "    for p in tqdm(train_dataset, total=int(len(train_dataset))):\n",
    "        Path(path + str(p[1])).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        image = p[0].numpy().transpose(1, 2, 0)\n",
    "        #image = cv2.bitwise_not(image)\n",
    "        image = 255 - image\n",
    "        cv2.imwrite(path + str(p[1]) +'/' + str(counter)+'.jpg', image)\n",
    "        #rotated.save(path + true_name + '/' + str(counter)+'.jpg')\n",
    "        #print(train_transform(p[0]))\n",
    "        train_transform(p[0])\n",
    "        counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee6a3c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]e:\\Programs\\Anaconda3\\envs\\MyPyTorch\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 123.30it/s]\n"
     ]
    }
   ],
   "source": [
    "resize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce943e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce15509d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ff07cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067aa4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c7c80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MathNet, self).__init__()\n",
    "        self.dropout_percentage = 0.25\n",
    "        \n",
    "        #Block 1     in: 28x28x1\n",
    "        #Block 1    out: 14x14x64\n",
    "        self.conv1_1 = torch.nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1_1 = torch.nn.BatchNorm2d(num_features=64)\n",
    "        self.conv1_2 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1_2 = torch.nn.BatchNorm2d(num_features=64)\n",
    "        self.conv1_3 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1_3 = torch.nn.BatchNorm2d(num_features=64)\n",
    "        self.act1  = torch.nn.ReLU()\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        self.dropout1 = nn.Dropout(p=self.dropout_percentage)\n",
    "        \n",
    "        #Block 2     in: 14x14x64\n",
    "        #Block 2    out: 7x7x128\n",
    "        self.conv2_1 = torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2_1 = torch.nn.BatchNorm2d(num_features=128)\n",
    "        self.conv2_2 = torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2_2 = torch.nn.BatchNorm2d(num_features=128)\n",
    "        self.conv2_3 = torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2_3 = torch.nn.BatchNorm2d(num_features=128)\n",
    "        self.act2  = torch.nn.ReLU()\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        self.dropout2 = nn.Dropout(p=self.dropout_percentage)\n",
    "        \n",
    "        #Block 2     in: 7x7x128\n",
    "        #Block 2    out: 1x1x256\n",
    "        self.conv3_1 = torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn3_1 = torch.nn.BatchNorm2d(num_features=256)\n",
    "        self.conv3_2 = torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn3_2 = torch.nn.BatchNorm2d(num_features=256)\n",
    "        self.conv3_3 = torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn3_3 = torch.nn.BatchNorm2d(num_features=256)\n",
    "        self.act3  = torch.nn.ReLU()\n",
    "        self.pool3 = torch.nn.AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
    "        self.dropout3 = nn.Dropout(p=self.dropout_percentage)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(256, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1_1(x)\n",
    "        x = self.bn1_1(x)\n",
    "        x = self.conv1_2(x)\n",
    "        x = self.bn1_2(x)\n",
    "        x = self.conv1_3(x)\n",
    "        x = self.bn1_3(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2_1(x)\n",
    "        x = self.bn2_1(x)\n",
    "        x = self.conv2_2(x)\n",
    "        x = self.bn2_2(x)\n",
    "        x = self.conv2_3(x)\n",
    "        x = self.bn2_3(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3_1(x)\n",
    "        x = self.bn3_1(x)\n",
    "        x = self.conv3_2(x)\n",
    "        x = self.bn3_2(x)\n",
    "        x = self.conv3_3(x)\n",
    "        x = self.bn3_3(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = x.view(x.size(0), x.size(1) * x.size(2) * x.size(3))\n",
    "        x = self.fc1(x)  \n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
