{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1f48c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2360c1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCHS = 32\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "435dfc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_size = (56, 56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf58412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(desired_size),\n",
    "    transforms.Grayscale(1)\n",
    "    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2df351f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAA4ADgBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APf6K88+NOu3WhfDu6ksbsW9zPIkIII3FWPzY/CvkQAsQAMk19ffBnRr/Q/hxaW2o27QTvK8ojbqFY5Ga9Aooor5y/aUZv7c0Ndx2/ZpDjtncK4r4P6Fdaz8RNNlhtBPbWcgmuSwBVF5wTn3r7CAwMCiiiivnH9pT/kP6H/16yf+hCj9mv8A5D+uf9esf/oRr6Ooooor5x/aU/5D+h/9esn/AKEKP2a/+Q/rn/XrH/6Ea+jqKKKK+cf2lP8AkP6H/wBesn/oQo/Zr/5D+uf9esf/AKEa+jqKKKK+cf2lP+Q/of8A16yf+hCmfs2yxp4i1mNpFV3tk2qTgthucetfSNFFFFfOP7Sn/If0P/r1k/8AQq8Z0zVL3RtSg1DT7h4LqBt0cidQa+yPhx4rm8Z+DLXV7iBYZyzRSBTkFl4JH1rrKK//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADgAAAA4CAAAAACN7WTCAAABZElEQVR4Ae1WsU7DUAy0Xzuzd4EJvgE6gWCrBN9S+gdsDKgfQgfEAAMgdQPxCVmLygiCDRobImFjlHupYETJYudy9+y75YUUPfJ21KGvp3tcIkrXvseqr1eXEgHQQyFNRzM1suIjkn2PVYpH10U89lDIi9KFnNxt1BEURgb3NuKr98Cjkrgt5q3hNjs7NvWo5XnSN25aPX2vMyqE6nB5tmYGuD95kjqjQsCqVMwtmjQ46NjwuOZnb2dHuBQTcjdldFAYD8n2aKJPYSabXTsACZ3EvfXcpnBVs8ib452sEKVqE9NgP5cpTtWElNytQ940enQWaFohCMWgNhxLAtQ2HBCKQW04lgSobTggFIP+SzgLuynNWKhNHuXi/OVXV7mR9X54/f1TF6ZVbdONTPIw5t0VfJujVZ2pcnt4k/HZOJFIZye6B2eiicGNyt1oap4DvuTnoWLqvPijUNU9x5FLVo3Un/0HfR/LVxGzG6oAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=56x56>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = torchvision.datasets.ImageFolder('T:\\my_programs\\Math_recognition\\TRAIN\\images_contrast_to224', transform=train_transform)\n",
    "display(dataset[112][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "426fabf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_dict = dataset.class_to_idx\n",
    "classes = []\n",
    "for key in classes_dict.keys():\n",
    "    classes.append(str(key))\n",
    "classes.sort()\n",
    "#classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67725534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classes = ['!', '(', ')', '+', ',', '-', \n",
    "#            '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', \n",
    "#            'A', 'C', 'Delta', 'G', 'H', 'M', 'N', 'R', 'S', 'T', 'X', '[', ']', \n",
    "#            'alpha', 'ascii_124', 'b', 'beta', 'chi', 'cos', 'd', 'div', 'e', 'exists', \n",
    "#            'f', 'forall', 'forward_slash', 'gamma', 'geq', 'greater', 'gt', \n",
    "#            'i', 'in', 'infty', 'int', 'j', 'k', 'l', 'lambda', 'ldots', 'leq', \n",
    "#            'lim', 'log', 'lt', 'mu', 'neq', 'o', 'omega', 'p', 'phi', 'pi', 'pm', 'prime', \n",
    "#            'q', 'rightarrow', 'sigma', 'sin', 'sqrt', 'sum', 'tan', 'theta', 'times', \n",
    "#            'u', 'upsilon', 'v', 'w', 'y', 'z', '{', '}']\n",
    "\n",
    "NUM_CLASSES = len(classes)\n",
    "\n",
    "def map_pred(ind):\n",
    "    if ind < NUM_CLASSES:\n",
    "        return classes[ind]\n",
    "    return 'ERROR MAPPIMG'\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97dc7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=6, pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d5d837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29a26e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize():\n",
    "    path = \"TRAIN/images_resized_to\"+str(desired_size[0])+\"/\"\n",
    "    counter = 0\n",
    "    for p in tqdm(dataset, total=int(len(dataset))):\n",
    "        true_name = map_pred(p[1])\n",
    "        Path(path + true_name).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        #image = np.array(p[0])\n",
    "        #gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        #thresh = 250\n",
    "        #ret, thresh_img = cv2.threshold(image, thresh, 255, cv2.THRESH_BINARY)\n",
    "        #img_erode = cv2.erode(thresh_img, np.ones((3, 3), np.uint8), iterations=1)\n",
    "        \n",
    "        #cv2.imwrite(path + true_name + '/' + str(counter)+'.jpg', img_erode)\n",
    "        #rotated.save(path + true_name + '/' + str(counter)+'.jpg')\n",
    "        train_transform(p[0]).save(path + true_name +'/' + str(counter)+'.jpg')\n",
    "        counter+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4be8f090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 338219/338219 [21:57<00:00, 256.78it/s]\n"
     ]
    }
   ],
   "source": [
    "resize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f4f491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy():\n",
    "    counter = 0\n",
    "    true_name = ''\n",
    "    classes = []\n",
    "    for p in dataset:\n",
    "        #print(p)\n",
    "        true_name1 = str(p[1])#map_pred(p[1])\n",
    "        if true_name != true_name1:\n",
    "            true_name = true_name1\n",
    "            classes.append(true_name)\n",
    "            Path(\"dummy\"+str(desired_size[0])+\"/\"+true_name).mkdir(parents=True, exist_ok=True)\n",
    "            p[0].save(\"dummy\"+str(desired_size[0])+\"/\" + true_name + '/' + str(counter)+'.jpg')\n",
    "            \n",
    "        if counter % 1000 == 0:\n",
    "            print(len(dataset)-counter)\n",
    "        #Path(\"new/\"+str(p[1])).mkdir(parents=True, exist_ok=True)\n",
    "        #torch.save(p[0], 'new/'+str(p[1]) + str(counter)+'.jpg')\n",
    "        counter+=1\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2dff414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_contrast(x, factor):\n",
    "    return transforms.functional.adjust_contrast(x, factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed63abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast():\n",
    "    path = \"images_contrast_to\"+str(desired_size[0])+\"/\"\n",
    "    counter = 0\n",
    "    for p in tqdm(dataset, total=int(len(dataset))):\n",
    "        true_name = map_pred(p[1])\n",
    "        Path(path + true_name).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        #display(p[0])\n",
    "        image = add_contrast(p[0], 3)\n",
    "        \n",
    "        #display(image)\n",
    "        \n",
    "        image.save(path + true_name + '/' + str(counter)+'.jpg')\n",
    "#         Path(\"new/\"+str(p[1])).mkdir(parents=True, exist_ok=True)\n",
    "#         torch.save(p[0], 'new/'+str(p[1]) + str(counter)+'.jpg')\n",
    "        counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "797d3267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 173/173 [00:01<00:00, 139.67it/s]\n"
     ]
    }
   ],
   "source": [
    "contrast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d000b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1dbce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be31384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa686fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', ')', ',', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'F', 'X', 'Y', 'Z', 'alpha', 'and', 'beta', 'delta', 'exists', 'forall', 'gamma', 'lambda', 'mu', 'not', 'omega', 'or', 'phi', 'pi', 'psi', 'rightarrow', 'sigma', 'tau', 'theta', 'upsilon']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root, dirs, files = os.walk(\"F:\\my_programs\\python\\image_recognition\\\\all_dataset\").__next__()\n",
    "\n",
    "dirs.sort()\n",
    "print(dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f1ec99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd3828b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8780231d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd33967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce943e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c7c80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MathNet, self).__init__()\n",
    "        self.dropout_percentage = 0.25\n",
    "        \n",
    "        #Block 1     in: 28x28x1\n",
    "        #Block 1    out: 14x14x64\n",
    "        self.conv1_1 = torch.nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1_1 = torch.nn.BatchNorm2d(num_features=64)\n",
    "        self.conv1_2 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1_2 = torch.nn.BatchNorm2d(num_features=64)\n",
    "        self.conv1_3 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1_3 = torch.nn.BatchNorm2d(num_features=64)\n",
    "        self.act1  = torch.nn.ReLU()\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        self.dropout1 = nn.Dropout(p=self.dropout_percentage)\n",
    "        \n",
    "        #Block 2     in: 14x14x64\n",
    "        #Block 2    out: 7x7x128\n",
    "        self.conv2_1 = torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2_1 = torch.nn.BatchNorm2d(num_features=128)\n",
    "        self.conv2_2 = torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2_2 = torch.nn.BatchNorm2d(num_features=128)\n",
    "        self.conv2_3 = torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2_3 = torch.nn.BatchNorm2d(num_features=128)\n",
    "        self.act2  = torch.nn.ReLU()\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        self.dropout2 = nn.Dropout(p=self.dropout_percentage)\n",
    "        \n",
    "        #Block 2     in: 7x7x128\n",
    "        #Block 2    out: 1x1x256\n",
    "        self.conv3_1 = torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn3_1 = torch.nn.BatchNorm2d(num_features=256)\n",
    "        self.conv3_2 = torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn3_2 = torch.nn.BatchNorm2d(num_features=256)\n",
    "        self.conv3_3 = torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn3_3 = torch.nn.BatchNorm2d(num_features=256)\n",
    "        self.act3  = torch.nn.ReLU()\n",
    "        self.pool3 = torch.nn.AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
    "        self.dropout3 = nn.Dropout(p=self.dropout_percentage)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(256, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1_1(x)\n",
    "        x = self.bn1_1(x)\n",
    "        x = self.conv1_2(x)\n",
    "        x = self.bn1_2(x)\n",
    "        x = self.conv1_3(x)\n",
    "        x = self.bn1_3(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2_1(x)\n",
    "        x = self.bn2_1(x)\n",
    "        x = self.conv2_2(x)\n",
    "        x = self.bn2_2(x)\n",
    "        x = self.conv2_3(x)\n",
    "        x = self.bn2_3(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3_1(x)\n",
    "        x = self.bn3_1(x)\n",
    "        x = self.conv3_2(x)\n",
    "        x = self.bn3_2(x)\n",
    "        x = self.conv3_3(x)\n",
    "        x = self.bn3_3(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = x.view(x.size(0), x.size(1) * x.size(2) * x.size(3))\n",
    "        x = self.fc1(x)  \n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
