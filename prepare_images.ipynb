{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1f48c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import models.MathNet as mnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2360c1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCHS = 32\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "435dfc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_size = (112, 112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf58412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(desired_size),\n",
    "    transforms.Grayscale(1)\n",
    "    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2df351f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCABwAHABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APf6KKKKSivN9Ru5fiNrV94ctiI9EsZTbauko2ySnqvlMM4GR3xXUeE/B2keC9PmstHSZIZZPNcSybzuxjr+FdBgZzS0UUUUUUUUVzfjbWW0jw3efY7lItWmhdbGPgvLLjgKp+8faq3gHRVsNAi1K4tXg1fU40n1EvkM8uOSVPA69ABXWUUtFFFFFFFFJ3rzG8/4rr4nfYG/0MeFLlLgOPn+0bwOMcbcY969O60tFFFFFFFFFFHvXmHgr/ksvjs9sQfyr0+iiiiiiiiiiisnxOzJ4U1dlJVhZykEHBHyGub+EKq/w00e5YBp5Ym8yUjLP87dT1Nd1RRRRRRRRRRRWR4q/wCRS1j/AK8pv/QDXO/B/wD5JXof/XJv/Q2ruaKKKKKKKKKKK88+LGq3unWOh29pOY4r/UktblQB+8iYEMp+tdtpWlWWiaZDp2nQCC0hGI4wSQoznv8AWrtFFFFFFFFFFFeYeIv+K3+IqeFJP9Fj0Uw6mtwvzGU8fIR2HPWvT6KKKKKKKKKKKK8w0b/k4bxB/wBguL+a16fRRRRRRRRRRRRXmGjf8nC+IP8AsFxfzWvT6KKKKKKKKKKKK8w0f/k4TxAP+oXF/Na9OpaKKKKKKKKKKK8w0f8A5OF8Qf8AYLi/mtendqWiiiiiiiiiiivPfiNpF9HJp2taHD5FxDdo+o3ULBHNsoywY9WXjpzXaaPq9jrulQalp03nWc4Jjk2kbhnHQ89qvUUUUUUUUUUUU10WRGR1DIwwQRkEV5/rHw3EeqT+INFv71NRjIltrDzwlpvA4UqBwp71R/4WVqHg/wDd/EWC2tJrj5rQacjShlH3t3PHJFekWV3Ff2VveQEmKeNZEyMHaRkVYoooor//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAGQUlEQVR4Ae1azYscRRSv6knIfmQ/JuwhZIORhFXJKQePuscgq4Le8g/4F3gUwZN68Sh487B40UtQQULQmyB4UFAxeBETdh1J1mUy2c1+dZe/96prprqzM/1eNUgOW7vTXVVd9X7v996rqu7qNk6eCp+4g89urVlrl245lJzL9w9wbEqZSU7WOGetMf7fGIe/5mQljbwYB8kkk04s/vcv9zJr7PSbV1AN+MKg2JiaTBCuk9GQRlYtii8Wz32NGl8X2jWdTzVqNKZB/s/O4NKpGVwFu9JMbIMx7UO1CtBBNts1t/bRe7dXP5tfMkVhOqWl6XKQO/asAmRXQRQ50t2/u7O8EMsFniCJAUl3SzENuRycXj4ChS7QP3Nv5CgGZOVpJFAwMkmA2MKREqwAYWKccLsJBx1giWrsoPdw4YXlLDZiM5bvHfeZoJgpKFzQGDTQ7PbbxTsvzXQRnn7owcwOOT8TTBJjpAxpGiFz4UenvQ0z+4wsSmrwUkAPxPTo4O3CgcJZUgJGYGVqCLWiFDCIYvEMGKQXPmisRTwJpjYxIBSlsCAq+UZ/+/nOnLcxjp6uv1bj82RRAVgUWUaAbveDb17+dH4RPMGSlKChSGNDEqgKwOA54/7d2D2/yCFDhL21CZ5LVDMhKQCD0xD8SCVMQJGQYzVSAAkRnYlPEYA9J1Q3sVQAepmm/9ege+0yrRAj2SFsyiYTT2wf0aHIeQ3+9upz6xtbuSuLoq5RIz3DvV4+fYE4RAwnUqpeVAAOg4bAAMeDoSpNUJL3GoUHzW0ULSMfCoBCEzEg4mIUGjTgk+CMeLVAuPux53Fi8wbdZWcxw0gcMU3kZ/iOJJI1MZtqxYpQVZRGPZNZppg0wtVnxQz9qGBi3rSJXhQDMhdnDvKjqRwTaXIaLtdNEsANnB6v/7h4ZXr12fQo9YtbNLmOy/qnp/4N++oDeoIa16yxXhc0niY5kn5JSezDSozgDi117OsYek4cpVgvkliKGQKLRkaJAr4VynLragAhlRAZKRFONZemmbDOXeFDRHyld7VUuTShoAAkr6WBxPgKHwIPAx6dMTuRC9PcqGCI0UBGBQ4ysdKqvAKQ5CbfygyVUgHaTgeIoNli7VcBDtVskVEEDfmNJrS0YAk6agB5akPHVogKQGse/Lx7fu3F0+04Nq6YUYPvVi6tbz3M403M6Kosq2BozOH24eluK4MqbvW90zEk/r/VgoIUeCHcEs8Kk9K01n6mEQMCDjt5mGmIGR38DK7mqZhpgvvUGJUOYoZEy2KDmxKTS4xWBcMSixHTQ0fM0B7+dK93PbvIsInsvMqyCcK5R28tvP5n/6BsnnyzL2aITcT+/tw8K9nmoPVhGyzuewLY2oR1AScmrVukdfnEpK1NWBdwYtK6RVqXn2aTpm87x2Z5mhnSzVP7JGaIm+4sw6Ftkt5i4FYmy/hpmxEJOe1OSgoI6cCM6KU+5ksB8VjhAQmU4jWNn2avbbiTyDRT8eSvgjyMN6R3YNrTjCJK4cLScfhKiFwaeVSeFfswEklIaWjoKAYkcuw4emqjzbaSbaSIKCs26WjcgRzv7qUhihmW6jNaICuiVGukBoQ9PR7btyZNUFSYNESKR0rEkwcNKQ8Q/1USDUHB10HHEVaY1O/RpDIL4AqTRns0PM2lRakCEMtTUHP01ceoRpgTm9TZ7vJcb69LXyMiRdi+QnqUv7AsNrd//bh495U0Sw71ETM02cXlv/846A97JmbEPiRiNM1wvDBY2vwtZ+jHPRb+8kNZXp5Yi+BP0QKpAbRTy/uP7011CYDnVB86AU9mY3nQ4JXTzubgk+9XP1yAWZ/4HpjpCTiqGM6uDAZ3LuOLwDCtykhVWikY0lu1/Zu/dKYAuLJ2hsONzenNW1q5Iv24QrlZ13jK8QlWUeT50VdLnU7nja0cL/OpjiqP8GYBGS42CZKblMlYl81cmDV2ZnOAD7zPoY4HB1/jMXMcp0qd2KRlYDq3ez835oePdqy5/v5Zj1S+qhG9sZEzBCKLnz2LzG93t63tcfhA/0BQ4kYVQ24M6c7cubmHpxvS9uprZ8qgqVhufKHJyeE6bwEP94ERKYcHn3exRN7o0zd8wwuh+dizxqSR1viUlJnSXQCHTXRtcvY/rNHoIzW0JbMAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=112x112>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = torchvision.datasets.ImageFolder('T:\\my_programs\\Math_recognition\\TRAIN\\dataset_balance', transform=train_transform)\n",
    "display(dataset[112][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "426fabf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_dict = dataset.class_to_idx\n",
    "classes = []\n",
    "for key in classes_dict.keys():\n",
    "    classes.append(str(key))\n",
    "classes.sort()\n",
    "#classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67725534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classes = ['!', '(', ')', '+', ',', '-', \n",
    "#            '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', \n",
    "#            'A', 'C', 'Delta', 'G', 'H', 'M', 'N', 'R', 'S', 'T', 'X', '[', ']', \n",
    "#            'alpha', 'ascii_124', 'b', 'beta', 'chi', 'cos', 'd', 'div', 'e', 'exists', \n",
    "#            'f', 'forall', 'forward_slash', 'gamma', 'geq', 'greater', 'gt', \n",
    "#            'i', 'in', 'infty', 'int', 'j', 'k', 'l', 'lambda', 'ldots', 'leq', \n",
    "#            'lim', 'log', 'lt', 'mu', 'neq', 'o', 'omega', 'p', 'phi', 'pi', 'pm', 'prime', \n",
    "#            'q', 'rightarrow', 'sigma', 'sin', 'sqrt', 'sum', 'tan', 'theta', 'times', \n",
    "#            'u', 'upsilon', 'v', 'w', 'y', 'z', '{', '}']\n",
    "\n",
    "NUM_CLASSES = len(classes)\n",
    "\n",
    "def map_pred(ind):\n",
    "    if ind < NUM_CLASSES:\n",
    "        return classes[ind]\n",
    "    return 'ERROR MAPPIMG'\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97dc7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=6, pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d5d837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29a26e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize():\n",
    "    path = \"TRAIN/images_resized_to\"+str(desired_size[0])+\"/\"\n",
    "    counter = 0\n",
    "    for p in tqdm(dataset, total=int(len(dataset))):\n",
    "        true_name = map_pred(p[1])\n",
    "        Path(path + true_name).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        #image = np.array(p[0])\n",
    "        #gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        #thresh = 250\n",
    "        #ret, thresh_img = cv2.threshold(image, thresh, 255, cv2.THRESH_BINARY)\n",
    "        #img_erode = cv2.erode(thresh_img, np.ones((3, 3), np.uint8), iterations=1)\n",
    "        \n",
    "        #cv2.imwrite(path + true_name + '/' + str(counter)+'.jpg', img_erode)\n",
    "        #rotated.save(path + true_name + '/' + str(counter)+'.jpg')\n",
    "        train_transform(p[0]).save(path + true_name +'/' + str(counter)+'.jpg')\n",
    "        counter+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4be8f090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133566/133566 [05:13<00:00, 426.03it/s]\n"
     ]
    }
   ],
   "source": [
    "resize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f4f491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy():\n",
    "    counter = 0\n",
    "    true_name = ''\n",
    "    classes = []\n",
    "    for p in dataset:\n",
    "        #print(p)\n",
    "        true_name1 = str(p[1])#map_pred(p[1])\n",
    "        if true_name != true_name1:\n",
    "            true_name = true_name1\n",
    "            classes.append(true_name)\n",
    "            Path(\"dummy\"+str(desired_size[0])+\"/\"+true_name).mkdir(parents=True, exist_ok=True)\n",
    "            p[0].save(\"dummy\"+str(desired_size[0])+\"/\" + true_name + '/' + str(counter)+'.jpg')\n",
    "            \n",
    "        if counter % 1000 == 0:\n",
    "            print(len(dataset)-counter)\n",
    "        #Path(\"new/\"+str(p[1])).mkdir(parents=True, exist_ok=True)\n",
    "        #torch.save(p[0], 'new/'+str(p[1]) + str(counter)+'.jpg')\n",
    "        counter+=1\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2dff414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_contrast(x, factor):\n",
    "    return transforms.functional.adjust_contrast(x, factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed63abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast():\n",
    "    path = \"images_contrast_to\"+str(desired_size[0])+\"/\"\n",
    "    counter = 0\n",
    "    for p in tqdm(dataset, total=int(len(dataset))):\n",
    "        true_name = map_pred(p[1])\n",
    "        Path(path + true_name).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        #display(p[0])\n",
    "        image = add_contrast(p[0], 3)\n",
    "        \n",
    "        #display(image)\n",
    "        \n",
    "        image.save(path + true_name + '/' + str(counter)+'.jpg')\n",
    "#         Path(\"new/\"+str(p[1])).mkdir(parents=True, exist_ok=True)\n",
    "#         torch.save(p[0], 'new/'+str(p[1]) + str(counter)+'.jpg')\n",
    "        counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "797d3267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 173/173 [00:01<00:00, 139.67it/s]\n"
     ]
    }
   ],
   "source": [
    "contrast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d000b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1dbce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be31384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa686fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', ')', ',', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'F', 'X', 'Y', 'Z', 'alpha', 'and', 'beta', 'delta', 'exists', 'forall', 'gamma', 'lambda', 'mu', 'not', 'omega', 'or', 'phi', 'pi', 'psi', 'rightarrow', 'sigma', 'tau', 'theta', 'upsilon']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root, dirs, files = os.walk(\"F:\\my_programs\\python\\image_recognition\\\\all_dataset\").__next__()\n",
    "\n",
    "dirs.sort()\n",
    "print(dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f1ec99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd3828b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afe09c32",
   "metadata": {},
   "source": [
    "EMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8780231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as T\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Создается класс для чтения dataset из csv файла \n",
    "class CustomDatasetEMNIST():\n",
    "    def __init__(self, csv_file_path, \n",
    "                 transform=None,\n",
    "                 nrows=None,\n",
    "                 skiprows=None) :\n",
    "        '''nrows - кол-во строк которое необходимо подгрузить,\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.data, self.targets, self.dataframe = self._load_data(csv_file_path=csv_file_path, nrows=nrows, skiprows=skiprows)\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self) :\n",
    "        return len(self.data) \n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "       \n",
    "        img = self.data[index]\n",
    "        #print(f'1. {img.shape=} {img}/n') \n",
    "        img = img.unsqueeze(0)\n",
    "        #print(f'2.{img.shape=} {img}/n') \n",
    "        targets = int(self.targets[index])\n",
    "        # Повернем и отразим изображение для лучшего восприятия\n",
    "        img = torchvision.transforms.functional.rotate(img=img, angle=-90)\n",
    "        img = torch.flip(img, dims=(2,))\n",
    "        \n",
    "        #print(f'3 {img.shape} {img} /n') \n",
    "        \n",
    "        if self.transform is not None:\n",
    "            #print(f'3. {img.shape} {img}') \n",
    "\n",
    "            img = self.transform(img)\n",
    "            #print(f'4.{img.shape} {img}') \n",
    "\n",
    "            \n",
    "        return img, targets\n",
    "    \n",
    "    \n",
    "    def _load_data(self,\n",
    "                  csv_file_path,\n",
    "                  nrows,\n",
    "                  skiprows=None):\n",
    "        \n",
    "        df = pd.read_csv(csv_file_path, nrows=nrows, skiprows=skiprows) \n",
    "        \n",
    "        targets = torch.tensor(df.iloc[:,0].values)\n",
    "        \n",
    "        data = df.iloc[:,1:].values.reshape((len(df), 28, 28))\n",
    "        \n",
    "        return torch.tensor(data).float(), targets, df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fd33967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зададим трансформацию для трейн и тест датасетов\n",
    "train_transform = T.Compose(\n",
    "    [\n",
    "        T.Resize([224, 224]),\n",
    "        T.RandomRotation(degrees=10),\n",
    "        T.Grayscale(1),\n",
    "        #transforms.ToPILImage()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60f92ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = CustomDatasetEMNIST(csv_file_path='E:\\TRAIN\\emnist/emnist-byclass-train.csv', \n",
    "                                    \n",
    "#                                     skiprows=50000,\n",
    "                                    nrows=1000, \n",
    "                                    transform=train_transform) \n",
    "\n",
    "valid_dataset = CustomDatasetEMNIST(csv_file_path='E:\\TRAIN\\emnist/emnist-byclass-test.csv',\n",
    "#                                    nrows=100000, \n",
    "                                    transform=train_transform) \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1500, shuffle=False, num_workers=6, pin_memory=True) \n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1500, shuffle=False, num_workers=6, pin_memory=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6cfcc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def resize():\n",
    "    path = \"E:\\TRAIN\\emnist2\\\\\"\n",
    "    counter = 0\n",
    "    for p in tqdm(train_dataset, total=int(len(train_dataset))):\n",
    "        Path(path + str(p[1])).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        image = p[0].numpy().transpose(1, 2, 0)\n",
    "        #image = cv2.bitwise_not(image)\n",
    "        image = 255 - image\n",
    "        cv2.imwrite(path + str(p[1]) +'/' + str(counter)+'.jpg', image)\n",
    "        #rotated.save(path + true_name + '/' + str(counter)+'.jpg')\n",
    "        #print(train_transform(p[0]))\n",
    "        train_transform(p[0])\n",
    "        counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee6a3c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]e:\\Programs\\Anaconda3\\envs\\MyPyTorch\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 123.30it/s]\n"
     ]
    }
   ],
   "source": [
    "resize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce943e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce15509d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ff07cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067aa4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c7c80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MathNet, self).__init__()\n",
    "        self.dropout_percentage = 0.25\n",
    "        \n",
    "        #Block 1     in: 28x28x1\n",
    "        #Block 1    out: 14x14x64\n",
    "        self.conv1_1 = torch.nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1_1 = torch.nn.BatchNorm2d(num_features=64)\n",
    "        self.conv1_2 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1_2 = torch.nn.BatchNorm2d(num_features=64)\n",
    "        self.conv1_3 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1_3 = torch.nn.BatchNorm2d(num_features=64)\n",
    "        self.act1  = torch.nn.ReLU()\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        self.dropout1 = nn.Dropout(p=self.dropout_percentage)\n",
    "        \n",
    "        #Block 2     in: 14x14x64\n",
    "        #Block 2    out: 7x7x128\n",
    "        self.conv2_1 = torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2_1 = torch.nn.BatchNorm2d(num_features=128)\n",
    "        self.conv2_2 = torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2_2 = torch.nn.BatchNorm2d(num_features=128)\n",
    "        self.conv2_3 = torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2_3 = torch.nn.BatchNorm2d(num_features=128)\n",
    "        self.act2  = torch.nn.ReLU()\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        self.dropout2 = nn.Dropout(p=self.dropout_percentage)\n",
    "        \n",
    "        #Block 2     in: 7x7x128\n",
    "        #Block 2    out: 1x1x256\n",
    "        self.conv3_1 = torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn3_1 = torch.nn.BatchNorm2d(num_features=256)\n",
    "        self.conv3_2 = torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn3_2 = torch.nn.BatchNorm2d(num_features=256)\n",
    "        self.conv3_3 = torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn3_3 = torch.nn.BatchNorm2d(num_features=256)\n",
    "        self.act3  = torch.nn.ReLU()\n",
    "        self.pool3 = torch.nn.AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
    "        self.dropout3 = nn.Dropout(p=self.dropout_percentage)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(256, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1_1(x)\n",
    "        x = self.bn1_1(x)\n",
    "        x = self.conv1_2(x)\n",
    "        x = self.bn1_2(x)\n",
    "        x = self.conv1_3(x)\n",
    "        x = self.bn1_3(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2_1(x)\n",
    "        x = self.bn2_1(x)\n",
    "        x = self.conv2_2(x)\n",
    "        x = self.bn2_2(x)\n",
    "        x = self.conv2_3(x)\n",
    "        x = self.bn2_3(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3_1(x)\n",
    "        x = self.bn3_1(x)\n",
    "        x = self.conv3_2(x)\n",
    "        x = self.bn3_2(x)\n",
    "        x = self.conv3_3(x)\n",
    "        x = self.bn3_3(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = x.view(x.size(0), x.size(1) * x.size(2) * x.size(3))\n",
    "        x = self.fc1(x)  \n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
