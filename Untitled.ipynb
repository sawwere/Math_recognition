{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a65edfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Tools for the HASY dataset.\n",
    "\n",
    "Type `./hasy_tools.py --help` for the command line tools and `help(hasy_tools)`\n",
    "in the interactive Python shell for the module options of hasy_tools.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb42eef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n",
    "                    level=logging.INFO,\n",
    "                    stream=sys.stdout)\n",
    "\n",
    "__version__ = \"v2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03a1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_csv(filepath, delimiter=',', quotechar=\"'\"):\n",
    "    \"\"\"\n",
    "    Load a CSV file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to a CSV file\n",
    "    delimiter : str, optional\n",
    "    quotechar : str, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of dicts : Each line of the CSV file is one element of the list.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    csv_dir = os.path.dirname(filepath)\n",
    "    with open(filepath, 'rb') as csvfile:\n",
    "        reader = csv.DictReader(csvfile,\n",
    "                                delimiter=delimiter,\n",
    "                                quotechar=quotechar)\n",
    "        for row in reader:\n",
    "            if 'path' in row:\n",
    "                row['path'] = os.path.abspath(os.path.join(csv_dir,\n",
    "                                                           row['path']))\n",
    "            data.append(row)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c610e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_index(csv_filepath):\n",
    "    \"\"\"\n",
    "    Generate an index 0...k for the k labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_filepath : str\n",
    "        Path to 'test.csv' or 'train.csv'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict : Maps a symbol_id as in test.csv and\n",
    "        train.csv to an integer in 0...k, where k is the total\n",
    "        number of unique labels.\n",
    "    \"\"\"\n",
    "    symbol_id2index = {}\n",
    "    data = _load_csv(csv_filepath)\n",
    "    i = 0\n",
    "    for item in data:\n",
    "        if item['symbol_id'] not in symbol_id2index:\n",
    "            symbol_id2index[item['symbol_id']] = i\n",
    "            i += 1\n",
    "    return symbol_id2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f8f92d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(csv_filepath, symbol_id2index, one_hot=True, flatten=False):\n",
    "    \"\"\"\n",
    "    Load the images into a 4D uint8 numpy array [index, y, x, depth].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_filepath : str\n",
    "        'test.csv' or 'train.csv'\n",
    "    symbol_id2index : dict\n",
    "        Dictionary generated by generate_index\n",
    "    one_hot : bool, optional\n",
    "        Make label vector as 1-hot encoding, otherwise index\n",
    "    flatten : bool, optional\n",
    "        Flatten feature vector\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    images, labels : Images is a 4D uint8 numpy array [index, y, x, depth]\n",
    "                     and labels is a 2D uint8 numpy array [index][1-hot enc].\n",
    "    \"\"\"\n",
    "    WIDTH, HEIGHT = 32, 32\n",
    "    dataset_path = os.path.dirname(csv_filepath)  # Main directory of HASY\n",
    "    data = _load_csv(csv_filepath)\n",
    "    if flatten:\n",
    "        images = np.zeros((len(data), WIDTH * HEIGHT))\n",
    "    else:\n",
    "        images = np.zeros((len(data), WIDTH, HEIGHT, 1))\n",
    "    labels = []\n",
    "    for i, data_item in enumerate(data):\n",
    "        fname = os.path.join(dataset_path, data_item['path'])\n",
    "        if flatten:\n",
    "            img = scipy.ndimage.imread(fname, flatten=False, mode='L')\n",
    "            images[i, :] = img.flatten()\n",
    "        else:\n",
    "            images[i, :, :, 0] = scipy.ndimage.imread(fname,\n",
    "                                                      flatten=False,\n",
    "                                                      mode='L')\n",
    "        label = symbol_id2index[data_item['symbol_id']]\n",
    "        labels.append(label)\n",
    "    data = images, np.array(labels)\n",
    "    if one_hot:\n",
    "        data = (data[0], np.eye(len(symbol_id2index))[data[1]])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb84388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_valid_png(filepath):\n",
    "    \"\"\"\n",
    "    Check if the PNG image is valid.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to a PNG image\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool : True if the PNG image is valid, otherwise False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        test = Image.open(filepath)\n",
    "        test.close()\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "476883bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _verify_all():\n",
    "    \"\"\"Verify all PNG files in the training and test directories.\"\"\"\n",
    "    for csv_data_path in ['classification-task/fold-1/test.csv',\n",
    "                          'classification-task/fold-1/train.csv']:\n",
    "        train_data = _load_csv(csv_data_path)\n",
    "        for data_item in train_data:\n",
    "            if not _is_valid_png(data_item['path']):\n",
    "                logging.info(\"%s is invalid.\" % data_item['path'])\n",
    "        logging.info(\"Checked %i items of %s.\" %\n",
    "                     (len(train_data), csv_data_path))\n",
    "\n",
    "\n",
    "def create_random_overview(img_src, x_images, y_images):\n",
    "    \"\"\"Create a random overview of images.\"\"\"\n",
    "    # Create canvas\n",
    "    background = Image.new('RGB',\n",
    "                           (35 * x_images, 35 * y_images),\n",
    "                           (255, 255, 255))\n",
    "    bg_w, bg_h = background.size\n",
    "    # Paste image on canvas\n",
    "    for x in range(x_images):\n",
    "        for y in range(y_images):\n",
    "            path = random.choice(img_src)['path']\n",
    "            img = Image.open(path, 'r')\n",
    "            img_w, img_h = img.size\n",
    "            offset = (35 * x, 35 * y)\n",
    "            background.paste(img, offset)\n",
    "    # Draw lines\n",
    "    draw = ImageDraw.Draw(background)\n",
    "    for y in range(y_images):  # horizontal lines\n",
    "        draw.line((0, 35 * y - 2, 35 * x_images, 35 * y - 2), fill=0)\n",
    "    for x in range(x_images):  # vertical lines\n",
    "        draw.line((35 * x - 2, 0, 35 * x - 2, 35 * y_images), fill=0)\n",
    "    # Store\n",
    "    background.save('hasy-overview.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d03920c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_colors(data, verbose=False):\n",
    "    \"\"\"\n",
    "    Get how often each color is used in data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict\n",
    "        with key 'path' pointing to an image\n",
    "    verbose : bool, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    color_count : dict\n",
    "        Maps a grayscale value (0..255) to how often it was in `data`\n",
    "    \"\"\"\n",
    "    color_count = {}\n",
    "    for i in range(256):\n",
    "        color_count[i] = 0\n",
    "    for i, data_item in enumerate(data):\n",
    "        if i % 1000 == 0 and i > 0 and verbose:\n",
    "            print(\"%i of %i done\" % (i, len(data)))\n",
    "        fname = os.path.join('.', data_item['path'])\n",
    "        img = scipy.ndimage.imread(fname, flatten=False, mode='L')\n",
    "        for row in img:\n",
    "            for pixel in row:\n",
    "                color_count[pixel] += 1\n",
    "    return color_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8ad388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_by_class(data):\n",
    "    \"\"\"\n",
    "    Organize `data` by class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : list of dicts\n",
    "        Each dict contains the key `symbol_id` which is the class label.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dbc : dict\n",
    "        mapping class labels to lists of dicts\n",
    "    \"\"\"\n",
    "    dbc = {}\n",
    "    for item in data:\n",
    "        if item['symbol_id'] in dbc:\n",
    "            dbc[item['symbol_id']].append(item)\n",
    "        else:\n",
    "            dbc[item['symbol_id']] = [item]\n",
    "    return dbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe9c1a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_color_statistics(csv_filepath, verbose=False):\n",
    "    \"\"\"\n",
    "    Count how often white / black is in the image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_filepath : str\n",
    "        'test.csv' or 'train.csv'\n",
    "    verbose : bool, optional\n",
    "    \"\"\"\n",
    "    symbolid2latex = _get_symbolid2latex()\n",
    "    data = _load_csv(csv_filepath)\n",
    "    black_level, classes = [], []\n",
    "    for symbol_id, elements in data_by_class(data).items():\n",
    "        colors = _get_colors(elements)\n",
    "        b = colors[0]\n",
    "        w = colors[255]\n",
    "        black_level.append(float(b) / (b + w))\n",
    "        classes.append(symbol_id)\n",
    "        if verbose:\n",
    "            print(\"%s:\\t%0.4f\" % (symbol_id, black_level[-1]))\n",
    "    print(\"Average black level: %0.4f\" % np.average(black_level))\n",
    "    print(\"Median black level: %0.4f\" % np.median(black_level))\n",
    "    print(\"Minimum black level: %0.4f (class: %s)\" %\n",
    "          (min(black_level),\n",
    "           [symbolid2latex[c]\n",
    "            for bl, c in zip(black_level, classes) if bl <= min(black_level)]))\n",
    "    print(\"Maximum black level: %0.4f (class: %s)\" %\n",
    "          (max(black_level),\n",
    "           [symbolid2latex[c]\n",
    "            for bl, c in zip(black_level, classes) if bl >= max(black_level)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cdea79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_symbolid2latex(csv_filepath='symbols.csv'):\n",
    "    \"\"\"Return a dict mapping symbol_ids to LaTeX code.\"\"\"\n",
    "    symbol_data = _load_csv(csv_filepath)\n",
    "    symbolid2latex = {}\n",
    "    for row in symbol_data:\n",
    "        symbolid2latex[row['symbol_id']] = row['latex']\n",
    "    return symbolid2latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41b2b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _analyze_class_distribution(csv_filepath,\n",
    "                                max_data=1000,\n",
    "                                bin_size=25):\n",
    "    \"\"\"Plot the distribution of training data over graphs.\"\"\"\n",
    "    symbol_id2index = generate_index(csv_filepath)\n",
    "    index2symbol_id = {}\n",
    "    for index, symbol_id in symbol_id2index.items():\n",
    "        index2symbol_id[symbol_id] = index\n",
    "    data, y = load_images(csv_filepath, symbol_id2index, one_hot=False)\n",
    "\n",
    "    data = {}\n",
    "    for el in y:\n",
    "        if el in data:\n",
    "            data[el] += 1\n",
    "        else:\n",
    "            data[el] = 1\n",
    "    classes = data\n",
    "    images = len(y)\n",
    "\n",
    "    # Create plot\n",
    "    print(\"Classes: %i\" % len(classes))\n",
    "    print(\"Images: %i\" % images)\n",
    "\n",
    "    class_counts = sorted([count for _, count in classes.items()])\n",
    "    print(\"\\tmin: %i\" % min(class_counts))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    # plt.title('HASY training data distribution')\n",
    "    plt.xlabel('Amount of available testing images')\n",
    "    plt.ylabel('Number of classes')\n",
    "\n",
    "    # Where we want the ticks, in pixel locations\n",
    "    ticks = [int(el) for el in list(np.linspace(0, 200, 21))]\n",
    "    # What those pixel locations correspond to in data coordinates.\n",
    "    # Also set the float format here\n",
    "    ax1.set_xticks(ticks)\n",
    "    labels = ax1.get_xticklabels()\n",
    "    plt.setp(labels, rotation=30)\n",
    "\n",
    "    min_examples = 0\n",
    "    ax1.hist(class_counts, bins=range(min_examples, max_data + 1, bin_size))\n",
    "    # plt.show()\n",
    "    filename = '{}.pdf'.format('data-dist')\n",
    "    plt.savefig(filename)\n",
    "    logging.info(\"Plot has been saved as {}\".format(filename))\n",
    "\n",
    "    symbolid2latex = _get_symbolid2latex()\n",
    "\n",
    "    top10 = sorted(classes.items(), key=lambda n: n[1], reverse=True)[:10]\n",
    "    top10_data = 0\n",
    "    for index, count in top10:\n",
    "        print(\"\\t%s:\\t%i\" % (symbolid2latex[index2symbol_id[index]], count))\n",
    "        top10_data += count\n",
    "    total_data = sum([count for index, count in classes.items()])\n",
    "    print(\"Top-10 has %i training data (%0.2f%% of total)\" %\n",
    "          (top10_data, float(top10_data) * 100.0 / total_data))\n",
    "    print(\"%i classes have more than %i data items.\" %\n",
    "          (sum([1 for _, count in classes.items() if count > max_data]),\n",
    "           max_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f235a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _analyze_pca(csv_filepath):\n",
    "    \"\"\"\n",
    "    Analyze how much data can be compressed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_filepath : str\n",
    "        Path relative to dataset_path to a CSV file which points to images\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import PCA\n",
    "    import itertools as it\n",
    "\n",
    "    symbol_id2index = generate_index(csv_filepath)\n",
    "    data, y = load_images(csv_filepath, symbol_id2index, one_hot=False)\n",
    "    data = data.reshape(data.shape[0], data.shape[1] * data.shape[2])\n",
    "    pca = PCA()\n",
    "    pca.fit(data)\n",
    "    sum_ = 0.0\n",
    "    done_values = [None, None, None]\n",
    "    done_points = [False, False, False]\n",
    "    chck_points = [0.9, 0.95, 0.99]\n",
    "    for counter, el in enumerate(pca.explained_variance_ratio_):\n",
    "        sum_ += el\n",
    "        for check_point, done, i in zip(chck_points, done_points, it.count()):\n",
    "            if not done and sum_ >= check_point:\n",
    "                done_points[i] = counter\n",
    "                done_values[i] = sum_\n",
    "    for components, variance in zip(done_points, done_values):\n",
    "        print(\"%i components explain %0.2f of the variance\" %\n",
    "              (components, variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49a3bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_euclidean_dist(e1, e2):\n",
    "    \"\"\"Calculate the euclidean distance between e1 and e2.\"\"\"\n",
    "    e1 = e1.flatten()\n",
    "    e2 = e2.flatten()\n",
    "    return sum([(el1 - el2)**2 for el1, el2 in zip(e1, e2)])**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ec8dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _inner_class_distance(data):\n",
    "    \"\"\"Measure the eucliden distances of one class to the mean image.\"\"\"\n",
    "    distances = []\n",
    "    mean_img = None\n",
    "    for e1 in data:\n",
    "        fname1 = os.path.join('.', e1['path'])\n",
    "        img1 = scipy.ndimage.imread(fname1, flatten=False, mode='L')\n",
    "        if mean_img is None:\n",
    "            mean_img = img1.tolist()\n",
    "        else:\n",
    "            mean_img += img1\n",
    "    mean_img = mean_img / float(len(data))\n",
    "    # mean_img = thresholdize(mean_img, 'auto')\n",
    "    scipy.misc.imshow(mean_img)\n",
    "    for e1 in data:\n",
    "        fname1 = os.path.join('.', e1['path'])\n",
    "        img1 = scipy.ndimage.imread(fname1, flatten=False, mode='L')\n",
    "        dist = _get_euclidean_dist(img1, mean_img)\n",
    "        distances.append(dist)\n",
    "\n",
    "    return (distances, mean_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea5c4ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholdize(img, threshold=0.5):\n",
    "    \"\"\"Create a black-and-white image from a grayscale image.\"\"\"\n",
    "    img_new = []\n",
    "    if threshold == 'auto':\n",
    "        img_flat = sorted(img.flatten())\n",
    "        threshold_ind = int(0.85 * len(img_flat))\n",
    "        threshold = img_flat[threshold_ind]\n",
    "    for row in img:\n",
    "        bla = []\n",
    "        for col in row:\n",
    "            if col > threshold:\n",
    "                bla.append(1)\n",
    "            else:\n",
    "                bla.append(0)\n",
    "        img_new.append(bla)\n",
    "    return np.array(img_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ee9cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _analyze_distances(csv_filepath):\n",
    "    \"\"\"Analyze the distance between elements of one class and class means.\"\"\"\n",
    "    symbolid2latex = _get_symbolid2latex()\n",
    "    data = _load_csv(csv_filepath)\n",
    "    data = data_by_class(data)\n",
    "    mean_imgs = []\n",
    "    for class_, data_class in data.items():\n",
    "        latex = symbolid2latex[class_]\n",
    "        d, mean_img = _inner_class_distance(data_class)\n",
    "        # scipy.misc.imshow(mean_img)\n",
    "        print(\"%s: min=%0.4f, avg=%0.4f, median=%0.4f max=%0.4f\" %\n",
    "              (latex, np.min(d), np.average(d), np.median(d), np.max(d)))\n",
    "        distarr = sorted([(label, mean_c, _get_euclidean_dist(mean_c,\n",
    "                                                              mean_img))\n",
    "                          for label, mean_c in mean_imgs],\n",
    "                         key=lambda n: n[2])\n",
    "        for label, mean_c, d in distarr:\n",
    "            print(\"\\t%s: %0.4f\" % (label, d))\n",
    "        mean_imgs.append((latex, mean_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3da58772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _analyze_variance(csv_filepath):\n",
    "    \"\"\"Calculate the variance of each pixel.\"\"\"\n",
    "    symbol_id2index = generate_index(csv_filepath)\n",
    "    data, y = load_images(csv_filepath, symbol_id2index, one_hot=False)\n",
    "    # Calculate mean\n",
    "    sum_ = np.zeros((32, 32))\n",
    "    for el in data:\n",
    "        el = np.squeeze(el)\n",
    "        sum_ += el\n",
    "    mean_ = sum_ / float(len(data))\n",
    "    scipy.misc.imshow(mean_)\n",
    "\n",
    "    # Calculate variance\n",
    "    centered_ = np.zeros((32, 32))\n",
    "    for el in data:\n",
    "        el = np.squeeze(el)\n",
    "        centered_ += (el - mean_)**2\n",
    "    centered_ = (1. / len(data)) * centered_**0.5\n",
    "    scipy.misc.imshow(centered_)\n",
    "    for row in list(centered_):\n",
    "        row = list(row)\n",
    "        print(\" \".join([\"%0.1f\" % nr for nr in row]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "305e0c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _analyze_correlation(csv_filepath):\n",
    "    \"\"\"\n",
    "    Analyze and visualize the correlation of features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_filepath : str\n",
    "        Path to a CSV file which points to images\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from matplotlib import pyplot as plt\n",
    "    from matplotlib import cm as cm\n",
    "\n",
    "    symbol_id2index = generate_index(csv_filepath)\n",
    "    data, y = load_images(csv_filepath,\n",
    "                          symbol_id2index,\n",
    "                          one_hot=False,\n",
    "                          flatten=True)\n",
    "    df = pd.DataFrame(data=data)\n",
    "\n",
    "    logging.info(\"Data loaded. Start correlation calculation. Takes 1.5h.\")\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "\n",
    "    # Where we want the ticks, in pixel locations\n",
    "    ticks = np.linspace(0, 1024, 17)\n",
    "    # What those pixel locations correspond to in data coordinates.\n",
    "    # Also set the float format here\n",
    "    ax1.set_xticks(ticks)\n",
    "    ax1.set_yticks(ticks)\n",
    "    labels = ax1.get_xticklabels()\n",
    "    plt.setp(labels, rotation=30)\n",
    "\n",
    "    cmap = cm.get_cmap('jet', 30)\n",
    "    cax = ax1.imshow(df.corr(), interpolation=\"nearest\", cmap=cmap)\n",
    "    ax1.grid(True)\n",
    "    # Add colorbar, make sure to specify tick locations to match desired\n",
    "    # ticklabels\n",
    "    fig.colorbar(cax, ticks=[-0.15, 0, 0.15, 0.30, 0.45, 0.60, 0.75, 0.90, 1])\n",
    "    filename = '{}.pdf'.format('feature-correlation')\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea15f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_stratified_split(csv_filepath, n_splits):\n",
    "    \"\"\"\n",
    "    Create a stratified split for the classification task.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_filepath : str\n",
    "        Path to a CSV file which points to images\n",
    "    n_splits : int\n",
    "        Number of splits to make\n",
    "    \"\"\"\n",
    "    from sklearn.cross_validation import StratifiedKFold\n",
    "    data = _load_csv(csv_filepath)\n",
    "    labels = [el['symbol_id'] for el in data]\n",
    "    skf = StratifiedKFold(labels, n_folds=n_splits)\n",
    "    i = 1\n",
    "    kdirectory = 'classification-task'\n",
    "    if not os.path.exists(kdirectory):\n",
    "            os.makedirs(kdirectory)\n",
    "    for train_index, test_index in skf:\n",
    "        print(\"Create fold %i\" % i)\n",
    "        directory = \"%s/fold-%i\" % (kdirectory, i)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        else:\n",
    "            print(\"Directory '%s' already exists. Please remove it.\" %\n",
    "                  directory)\n",
    "        i += 1\n",
    "        train = [data[el] for el in train_index]\n",
    "        test_ = [data[el] for el in test_index]\n",
    "        for dataset, name in [(train, 'train'), (test_, 'test')]:\n",
    "            with open(\"%s/%s.csv\" % (directory, name), 'wb') as csv_file:\n",
    "                csv_writer = csv.writer(csv_file)\n",
    "                csv_writer.writerow(('path', 'symbol_id', 'latex', 'user_id'))\n",
    "                for el in dataset:\n",
    "                    csv_writer.writerow((\"../../%s\" % el['path'],\n",
    "                                         el['symbol_id'],\n",
    "                                         el['latex'],\n",
    "                                         el['user_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bb1d6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_pair(r1_data, r2_data):\n",
    "    \"\"\"Create a pair for the verification test.\"\"\"\n",
    "    symbol_index = random.choice(r1_data.keys())\n",
    "    r1 = random.choice(r1_data[symbol_index])\n",
    "    is_same = random.choice([True, False])\n",
    "    if is_same:\n",
    "        symbol_index2 = symbol_index\n",
    "        r2 = random.choice(r1_data[symbol_index2])\n",
    "    else:\n",
    "        symbol_index2 = random.choice(r2_data.keys())\n",
    "        while symbol_index2 == symbol_index:\n",
    "            symbol_index2 = random.choice(r2_data.keys())\n",
    "        r2 = random.choice(r2_data[symbol_index2])\n",
    "    return (r1['path'], r2['path'], is_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb60a297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_verification_task(sample_size=32, test_size=0.05):\n",
    "    \"\"\"\n",
    "    Create the datasets for the verification task.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_size : int\n",
    "        Number of classes which will be taken completely\n",
    "    test_size : float in (0, 1)\n",
    "        Percentage of the remaining data to be taken to test\n",
    "    \"\"\"\n",
    "    # Get the data\n",
    "    data = _load_csv('hasy-data-labels.csv')\n",
    "    for el in data:\n",
    "        el['path'] = \"../hasy-data/\" + el['path'].split(\"hasy-data/\")[1]\n",
    "    data = sorted(data_by_class(data).items(),\n",
    "                  key=lambda n: len(n[1]),\n",
    "                  reverse=True)\n",
    "    symbolid2latex = _get_symbolid2latex()\n",
    "\n",
    "    # Get complete classes\n",
    "    random.seed(1337)\n",
    "    symbols = random.sample(range(len(data)), k=sample_size)\n",
    "    symbols = sorted(symbols, reverse=True)\n",
    "    test_data_excluded = []\n",
    "    for symbol_index in symbols:\n",
    "        # for class_label, items in data:\n",
    "        class_label, items = data.pop(symbol_index)\n",
    "        test_data_excluded += items\n",
    "        print(symbolid2latex[class_label])\n",
    "\n",
    "    # Get data from remaining classes\n",
    "    data_n = []\n",
    "    for class_label, items in data:\n",
    "        data_n = data_n + items\n",
    "    ys = [el['symbol_id'] for el in data_n]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_n,\n",
    "                                                        ys,\n",
    "                                                        test_size=test_size)\n",
    "\n",
    "    # Write the training / test data\n",
    "    print(\"Test data (excluded symbols) = %i\" % len(test_data_excluded))\n",
    "    print(\"Test data (included symbols) = %i\" % len(x_test))\n",
    "    print(\"Test data (total) = %i\" % (len(x_test) + len(test_data_excluded)))\n",
    "    kdirectory = 'verification-task'\n",
    "    if not os.path.exists(kdirectory):\n",
    "        os.makedirs(kdirectory)\n",
    "    with open(\"%s/train.csv\" % kdirectory, 'wb') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(('path', 'symbol_id', 'latex', 'user_id'))\n",
    "        for el in x_train:\n",
    "            csv_writer.writerow((el['path'],\n",
    "                                 el['symbol_id'],\n",
    "                                 el['latex'],\n",
    "                                 el['user_id']))\n",
    "\n",
    "    x_test_inc_class = data_by_class(x_test)\n",
    "    x_text_exc_class = data_by_class(test_data_excluded)\n",
    "    # V1: Both symbols belong to the training set (included symbols)\n",
    "    with open(\"%s/test-v1.csv\" % kdirectory, 'wb') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(('path1', 'path2', 'is_same'))\n",
    "        for i in range(100000):\n",
    "            test_data_tuple = _create_pair(x_test_inc_class, x_test_inc_class)\n",
    "            csv_writer.writerow(test_data_tuple)\n",
    "\n",
    "    # V2: r1 belongs to a symbol in the training set, but r2 might not\n",
    "    with open(\"%s/test-v2.csv\" % kdirectory, 'wb') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(('path1', 'path2', 'is_same'))\n",
    "        for i in range(100000):\n",
    "            test_data_tuple = _create_pair(x_test_inc_class, x_text_exc_class)\n",
    "            csv_writer.writerow(test_data_tuple)\n",
    "\n",
    "    # V3: r1 and r2 both don't belong to symbols in the training set\n",
    "    with open(\"%s/test-v3.csv\" % kdirectory, 'wb') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(('path1', 'path2', 'is_same'))\n",
    "        for i in range(100000):\n",
    "            test_data_tuple = _create_pair(x_text_exc_class, x_text_exc_class)\n",
    "            csv_writer.writerow(test_data_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ac10577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_users(csv_filepath):\n",
    "    \"\"\"\n",
    "    Count the number of users who contributed to the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_filepath : str\n",
    "        Path to a CSV file which points to images\n",
    "    \"\"\"\n",
    "    data = _load_csv(csv_filepath)\n",
    "    user_ids = {}\n",
    "    for el in data:\n",
    "        if el['user_id'] not in user_ids:\n",
    "            user_ids[el['user_id']] = [el['path']]\n",
    "        else:\n",
    "            user_ids[el['user_id']].append(el['path'])\n",
    "    max_els = 0\n",
    "    max_user = 0\n",
    "    for user_id, elements in user_ids.items():\n",
    "        if len(elements) > max_els:\n",
    "            max_els = len(elements)\n",
    "            max_user = user_id\n",
    "    print(\"Dataset has %i users.\" % len(user_ids))\n",
    "    print(\"User %s created most (%i elements, %0.2f%%)\" %\n",
    "          (max_user, max_els, float(max_els) / len(data) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aad56440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_parser():\n",
    "    \"\"\"Get parser object for hasy_tools.py.\"\"\"\n",
    "    import argparse\n",
    "    from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "    parser = ArgumentParser(description=__doc__,\n",
    "                            formatter_class=ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\"--dataset\",\n",
    "                        dest=\"dataset\",\n",
    "                        default='classification-task/fold-1/train.csv',\n",
    "                        help=\"specify which data to use\")\n",
    "    parser.add_argument(\"--verify\",\n",
    "                        dest=\"verify\",\n",
    "                        action=\"store_true\",\n",
    "                        default=False,\n",
    "                        help=\"verify PNG files\")\n",
    "    parser.add_argument(\"--overview\",\n",
    "                        dest=\"overview\",\n",
    "                        action=\"store_true\",\n",
    "                        default=False,\n",
    "                        help=\"Get overview of data\")\n",
    "    parser.add_argument(\"--analyze_color\",\n",
    "                        dest=\"analyze_color\",\n",
    "                        action=\"store_true\",\n",
    "                        default=False,\n",
    "                        help=\"Analyze the color distribution\")\n",
    "    parser.add_argument(\"--class_distribution\",\n",
    "                        dest=\"class_distribution\",\n",
    "                        action=\"store_true\",\n",
    "                        default=False,\n",
    "                        help=\"Analyze the class distribution\")\n",
    "    parser.add_argument(\"--distances\",\n",
    "                        dest=\"distances\",\n",
    "                        action=\"store_true\",\n",
    "                        default=False,\n",
    "                        help=\"Analyze the euclidean distance distribution\")\n",
    "    parser.add_argument(\"--pca\",\n",
    "                        dest=\"pca\",\n",
    "                        action=\"store_true\",\n",
    "                        default=False,\n",
    "                        help=(\"Show how many principal components explain \"\n",
    "                              \"90%% / 95%% / 99%% of the variance\"))\n",
    "    parser.add_argument(\"--variance\",\n",
    "                        dest=\"variance\",\n",
    "                        action=\"store_true\",\n",
    "                        default=False,\n",
    "                        help=\"Analyze the variance of features\")\n",
    "    parser.add_argument(\"--correlation\",\n",
    "                        dest=\"correlation\",\n",
    "                        action=\"store_true\",\n",
    "                        default=False,\n",
    "                        help=\"Analyze the correlation of features\")\n",
    "    parser.add_argument(\"--create-classification-task\",\n",
    "                        dest=\"create_folds\",\n",
    "                        action=\"store_true\",\n",
    "                        default=False,\n",
    "                        help=argparse.SUPPRESS)\n",
    "    parser.add_argument(\"--create-verification-task\",\n",
    "                        dest=\"create_verification_task\",\n",
    "                        action=\"store_true\",\n",
    "                        default=False,\n",
    "                        help=argparse.SUPPRESS)\n",
    "    parser.add_argument(\"--count-users\",\n",
    "                        dest=\"count_users\",\n",
    "                        action=\"store_true\",\n",
    "                        default=False,\n",
    "                        help=\"Count how many different users have created \"\n",
    "                             \"the dataset\")\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b18c714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61814eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a71d664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--dataset DATASET] [--verify] [--overview] [--analyze_color] [--class_distribution]\n",
      "                             [--distances] [--pca] [--variance] [--correlation] [--count-users]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\sawer\\AppData\\Roaming\\jupyter\\runtime\\kernel-b6adcc6b-0bc9-4600-910e-d1c20d3e6506.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Programs\\Anaconda3\\envs\\My_PyTorch\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = _get_parser().parse_args()\n",
    "    if args.verify:\n",
    "        _verify_all()\n",
    "    if args.overview:\n",
    "        img_src = _load_csv(args.dataset)\n",
    "        create_random_overview(img_src, x_images=10, y_images=10)\n",
    "    if args.analyze_color:\n",
    "        _get_color_statistics(csv_filepath=args.dataset)\n",
    "    if args.class_distribution:\n",
    "        _analyze_class_distribution(csv_filepath=args.dataset,\n",
    "                                    max_data=200,\n",
    "                                    bin_size=5)\n",
    "    if args.pca:\n",
    "        _analyze_pca(csv_filepath=args.dataset)\n",
    "    if args.distances:\n",
    "        _analyze_distances(csv_filepath=args.dataset)\n",
    "    if args.variance:\n",
    "        _analyze_variance(csv_filepath=args.dataset)\n",
    "    if args.correlation:\n",
    "        _analyze_correlation(csv_filepath=args.dataset)\n",
    "    if args.create_folds:\n",
    "        _create_stratified_split(args.dataset, int(args.create_folds))\n",
    "    if args.count_users:\n",
    "        _count_users(csv_filepath=args.dataset)\n",
    "    if args.create_verification_task:\n",
    "        _create_verification_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4777b104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
